# VelocityUCL - ATLAS pipeline

NERC Velocity Project analyses using Sanger genomes 

Pipeline set up on UCL servers.

Here I'll curate the variant calling pipeline and analyses undertaken using the Lepidoptera genomes generated by Sanger in 2019/2020.

Darwin Tree of Life (DToL) data - [live results](https://github.com/darwintreeoflife/darwintreeoflife.data/tree/master/species)

## Species status

|Species||path|
|--------|--------|--------|
|AdapterRemoval|2.3.2|/SAN/ugi/LepGenomics/Software/adapterremoval-2.3.2/build/AdapterRemoval|


## [Skip to pipeline](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/README.md#atlas)


## General information for working on the UCL server

See [here](https://hpc.cs.ucl.ac.uk/full-guide/) for a general guide of the CS server

For support email: cluster-support [at] ucl.ac.uk

To apply for access here:  https://hpc.cs.ucl.ac.uk/account-form

You'll need your UCL username and password


Login
```
#We're biosciences so we're working on the pchuckle node
#staff=tails
#students=knuckles

ssh -l username -J username@tails.cs.ucl.ac.uk pchuckle

```

Script examples
```
/share/apps/examples
```


Shared storage folder - location of all data and shared scripts. Keep an eye on the data storage (see below) as there will be multiple people using the folder. 

```
/SAN/ugi/LepGenomics (15Tb)

#Scripts for Velocity pipeline (you'll be working with the scripts in the pipeline folder)
/SAN/ugi/LepGenomics/VelocityPipeline

#If you add any files to this shared folder please make sure you change permissions so that everyone can access them: 
chmod g+wr filename 
chmod g+wr folder
```

Check space left in shared folder
```
quota -s |grep -A 2 LepGenomics
```


Any scripts or commands should be submitted to the queue or run in the interactive node - never in the login node. 

Create an interactive node like this:
```
qrsh -l tmem=8G, h_vmem=8G
```

The less resources you ask for, the sooner the node will start. It also depends on how busy the server is in general (check with "top") 



Copy data to the server from computer

*see TransferData.md for transfer from UoB to UCL server*
```
#1. From your computer
#Port Forwarding with scp (from https://hpc.cs.ucl.ac.uk/ssh-scp/)

If you are trying to copy data to and from the cluster from outside the Computer Science department, it might be necessary to set up a port forward so that you connect to a login node via a jump node.  The command to set this up is

ssh -L 2222:<login node>:22 <username>@<gateway>

For example, if your username was alice and you wanted to connect to the login node called ‘pchuckle’ via the tails gateway server, the command would be:

ssh -L 2222:pchuckle.cs.ucl.ac.uk:22 alice@tails.cs.ucl.ac.uk

Once you have set this up, anything you send to local port 2222 will be forwarded to port 22 on comic via tails (port 22 being the standard port for ssh).

At this point, leave the previous command (ssh -L) running, and open another terminal. You can now scp your data by doing the following:

scp -P 2222 /path/to/file <user name>@localhost:~/path/to/destination

e.g. copy to the shared folder

scp -P 2222 *fastq.gz ajansen@localhost:/SAN/ugi/LepGenomics/C3_Aricia_agestis

This will copy the file data.txt in the user alice’s home directory on their local machine to their home directory on the cluster, via the machine tails.
```

Specifically: 
```
#At destination
#open window1 and activate port forwarding: 

ssh -l ajansen -L 3000:morecambe.cs.ucl.ac.uk:22 ajansen@tails.cs.ucl.ac.uk

#open window2 and rsync files of interest to the local directory:

rsync -auve "ssh -p 3000" $i ajansen@localhost:/SAN/ugi/LepGenomics/C3_Aricia_agestis/04_diploSHIC_simulations_AJvR/job0022*fvec .

```


### Check that files have transferred properly

We can use md5sum to check if files are the same at origin and destination: 

*mac uses md5 instead of md5sum. We can use an alias to use the md5sum commands below: alias md5sum='md5 -r'*
```
#Create md5sum hashes for files at origin

md5sum file1.txt file2.txt file3.txt > hashes

#copy the hashes file to the destination (either cut and paste or cat)
#check that the hashes are the same

md5sum --check hashes
file1.txt: OK
file2.txt: OK
file3.txt: OK

```



## Software required

*Most software can be found in /share/apps OR /share/apps/genomics*

|Software|version|path|
|--------|--------|--------|
|AdapterRemoval|2.3.2|/SAN/ugi/LepGenomics/Software/adapterremoval-2.3.2/build/AdapterRemoval|
|ANGSD|0.935|/share/apps/genomics/angsd-0.935/bin/angsd|
|ATLAS| 0.9 ** latest version|/share/apps/genomics/atlas-0.9/atlas|
|ATLAS| 1.0** older version| /share/apps/genomics/atlas-1.0/atlas|
|bamtools|2.5.1| /share/apps/genomics/bamtools-2.5.1/bin/bamtools|
|BBmap|38.59| /share/apps/genomics/bbmap-38.59/bbmap.sh|
|bcftools|1.9| /share/apps/genomics/bcftools-1.9/bin/bcftools|
|BWA|0.7.17| /share/apps/genomics/bwa-0.7.17/bwa|
|Cutadapt | |/share/apps/python/bin/cutadapt| 
|Cutadapt|2.5|/share/apps/genomics/cutadapt-2.5/bin/cutadapt|
|discoal||/share/apps/genomics/discoal/discoal|
|fastQC|0.11.8| /share/apps/genomics/FastQC-0.11.8/fastqc|
|gatk|3.7.0|/share/apps/genomics/GenomeAnalysisTK-3.7/GenomeAnalysisTK.jar|
|gatk|3.8.1|/share/apps/genomics/GenomeAnalysisTK-3.8.1.0/GenomeAnalysisTK.jar|
|gcc|9.2.0|/share/apps/gcc-9.2.0/bin/gcc|
|GenomeAnalysisTK.jar|3.7| /share/apps/genomics/GenomeAnalysisTK-3.7/GenomeAnalysisTK.jar|
|java|| /share/apps/java/bin/java|
|MapDamage|2.1.1| /share/apps/python-3.8.5-shared/bin/mapDamage **export python and R to PATH. See below**|
|perl ||/share/apps/perl/bin/perl|
|picard |2.20.3|/share/apps/genomics/picard-2.20.3/bin/picard.jar|
|plink2 |v.2|/share/apps/genomics/plink-2.0/bin/plink2|
|python3 |3|/share/apps/python/bin/python3|
|R|4.0.3| /share/apps/R-4.0.3/bin/R|
|samtools|1.9| /share/apps/genomics/samtools-1.9/bin/samtools|
|Trimmomatic|0.39|/SAN/ugi/LepGenomics/Software/Trimmomatic-0.39|
|vcflib|1.0.0| /share/apps/genomics/vcflib-1.0.0/bin|
|vcftools|0.1.13| /share/apps/genomics/vcftools-0.1.13/bin/vcftools|


For some software we have to export the path to the software and their libraries to our PATH: 

ATLAS
```
export LD_LIBRARY_PATH=/share/apps/openblas-0.3.6/lib:/share/apps/armadillo-9.100.5/lib64:$LD_LIBRARY_PATH
```

bwa
```
export PATH=/share/apps/genomics/bwa-0.7.17/bwa:$PATH
```

perl
```
export PATH=/share/apps/perl-5.30.0/bin:$PATH
```

python
```
export PATH=/share/apps/python-3.8.5-shared/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/python-3.8.5-shared/lib:$LD_LIBRARY_PATH


export PATH=/share/apps/python-3.6.4-shared/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/python-3.6.4-shared/lib:$LD_LIBRARY_PATH

```

java
```
export PATH=/share/apps/java/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/java/lib:$LD_LIBRARY_PATH
```

mapDamage
```
# Software
##python
export PATH=/share/apps/python-3.8.5-shared/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/python-3.8.5-shared/lib:$LD_LIBRARY_PATH

##R
export PATH=/share/apps/R-4.0.3/bin:$PATH

##mapDamage
mapDamage="/share/apps/python-3.8.5-shared/bin/mapDamage"

```


samtools
```
export PATH=/share/apps/genomics/samtools-1.9/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/genomics/samtools-1.9/lib:$LD_LIBRARY_PATH

```




### Note on renaming files

Liverpool sequenced raw data is named with digits and a dash before the sample names. e.g. 33-AH-01-1900-47_191121_L001_R2.fastq.gz
The easiest way to rename them is with the Perl rename (note that the native linux rename works the same as mv and is not so useful in this case). 
Create a copy of the [prename.pl](https://gist.github.com/rocarvaj/6abf7dd1e083963a596430ac43f88e34) script in a folder in your home directory called "scripts". 
```
cd
mkdir scripts
nano prename.pl
#paste a copy of the script found at the link above
```

Run the script using perl installed in the shared server folder: 
```
/share/apps/perl-5.30.0/bin/perl5.30.0 ../../prename.pl --help
```


This works with the sed syntax. e.g. this will remove numbers plus dash from the start of the file names: 

Replace USERNAME with your username
```
/share/apps/perl-5.30.0/bin/perl5.30.0 /home/USERNAME/prename.pl 's/^[0-9]+-//' *

#e.g. This will delete all leading numbers and the dash:
/share/apps/perl-5.30.0/bin/perl5.30.0 /home/ajansen/prename.pl 's/^[0-9]+-//' *
```


## Data

See [here](https://github.com/alexjvr1/VelocityUCL/blob/main/RawDataCheck.md) for sample management for Modern3. 




# ATLAS

[ATLAS](https://bitbucket.org/wegmannlab/atlas/wiki/Home) is a pipeline developed in Daniel Wegmann's lab to process raw bam files from ancient DNA to obtain accurate variant calls and estimates of genetic diversity. 
The pipeline takes into account post-mortem damage (PMD) and low sequence coverage, and recovers variatns more accurately than the state-of-the-art method: MapDamage + GATK. 

ATLAS can be used to estimate genetic diversity. We can also generate input for ANGSD using ATLAS. 


## Test-species: 

E3 Aphantopus hyperantus

I'm testing the raw data pre-processing and ATLAS pipeline on the UCL server (CS) 

Since many of the steps in this pipeline run fairly quickly we'll submit these commands sequentially rather than as an array. Too many small jobs can slow down or crash jobs on the server, so arrays should only be used for bigger jobs >1 hour each. Previous scripts written as arrays are kept here: [smallARRAYscripts](https://github.com/alexjvr1/VelocityUCL/tree/main/ATLAS/Scripts/smallARRAYscripts)



## Running Pipeline: 
### 0. Concat raw museum samples 

33 individuals from each museum species has been sequenced twice to increase sequence depth. We're concatenating these raw data together, then moving all samples to a folder called 01a_raw_museum_FINAL



Use these three scripts in this order: 

[00a_CreateInputs.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/00a_CreateInputs.sh)

[00b_ConcatFastq.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/00b_ConcatFastq.sh)

[00c_CollateAllMusSamples.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/00c_CollateAllMusSamples.sh)


These take ~40minutes to run for E3



### 1. Remove adapter sequence using Cutadapt

We're using a script for each population. Run these in the working directory to create the submission script in each case: 

```
./script.sh
```

[01a_MUS_cutadapt_filtering_trimming.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/01a_MUS_cutadapt_filtering_trimming.sh)

[01a_MODC_cutadapt_filtering_trimming.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/01a_MODC_cutadapt_filtering_trimming.sh)

[01a_MODE_cutadapt_filtering_trimming.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/01a_MODE_cutadapt_filtering_trimming.sh)




### 2a. Map to Reference genome

Note that to use ATLAS correctly we will map unmerged reads.

ATLAS has an in-built function to annotate and merge these reads after mapping, and uses this sequence information for the PMD recalibration step. 


[02a_MapwithBWAmem.ARRAY_MUS_forATLAS.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/02a_MapwithBWAmem.ARRAY_MUS_forATLAS.sh)

[02a_MapwithBWAmem.ARRAY_MODC_forATLAS.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/02a_MapwithBWAmem.ARRAY_MODC_forATLAS.sh)

[02a_MapwithBWAmem.ARRAY_MODE_forATLAS.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/02a_MapwithBWAmem.ARRAY_MODE_forATLAS.sh)




### 2b Process bam files before using ATLAS

Step1. Add Read groups

Step2. Mark Duplicates

Step3. Local Realignment

Step4. Validate sam file



We're using GATK3.8 for the local realignment. First we need to create a dictionary for the reference genome if this is not available yet.

I'm running this in the interactive screen: 
```
qrsh -l tmem=34G,h_vmem=34G

cd /SAN/ugi/LepGenomics/E3_Aphantopus_hyperantus/RefGenome

export PATH=/share/apps/java/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/java/lib:$LD_LIBRARY_PATH
PICARD=/share/apps/genomics/picard-2.20.3/bin/picard.jar

java -jar $PICARD CreateSequenceDictionary R=GCA_902806685.1_iAphHyp1.1_genomic.fna O=GCA_902806685.1_iAphHyp1.1_genomic.fna.dict

```

Then run the processing scripts. There is a script for each of the populations. The input files and RG information is changed for each: 

[02b_processbams_MUS.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/02b_processbams_MUS.sh)

[02b_processbams_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/02b_processbams_MODC.sh)

[02b_processbams_MODE.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/02b_processbams_MODE.sh)



Outputs are written to the 02a_mapped_* folders. The final validation files are named ${NAME}.validatesam. If there are no errors they'll simply say: "No errors found"

e.g.
```
cd 02a_mapped_museum
cat *validatesam

No errors found
No errors found
No errors found

```

If any errors are found at this point they need to be corrected. Run ValidateSam on previous versions of the bam file to establish where the error started. 

eg. 

MATE_NOT_FOUND error means sequences were removed at one step in the processing. Sequences should be marked but not removed if they are problematic (e.g. mark duplicates rather than remove duplicates). 

Mate name or information errors can be corrected with [FixMateInformation](https://gatk.broadinstitute.org/hc/en-us/articles/360036713471-FixMateInformation-Picard-)

ERROR:MISMATCH_MATE_ALIGNMENT_START	1

ERROR:MISMATCH_MATE_CIGAR_STRING	1

e.g for E3 MODE
```
pwd
/SAN/ugi/LepGenomics/E3_Aphantopus_hyperantus/02a_mapped_modern_exp

cat *validatesam

#Most of the files have "No errors found", but for four samples we have: 
## HISTOGRAM	java.lang.String
Error Type	Count
ERROR:MISMATCH_MATE_CIGAR_STRING	1

#To find the files: 
grep "MISMATCH_MATE_CIGAR_STRING" *validatesam

AH-02-2019-47.validatesam:ERROR:MISMATCH_MATE_CIGAR_STRING	1
AH-02-2019-48.validatesam:ERROR:MISMATCH_MATE_CIGAR_STRING	2
AH-02-2019-58.validatesam:ERROR:MISMATCH_MATE_CIGAR_STRING	1
AH-02-2019-69.validatesam:ERROR:MISMATCH_MATE_CIGAR_STRING	1

#Then we can run FixMateInformation for these four samples

#Set path
export PATH=/share/apps/java/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/java/lib:$LD_LIBRARY_PATH
PICARD=/share/apps/genomics/picard-2.20.3/bin/picard.jar

#Run this for each of the four samples. 
java -jar $PICARD FixMateInformation \
       I=AH-02-2019-47.realn.bam \
       O=AH-02-2019-47.realn.fixed_mate.bam \
       ADD_MATE_CIGAR=true

#Run ValidateSam on the fixed files to check that this has worked. 

```


Rerun ValidateSam on the final bams to make sure there are no errors. 





## ATLAS

The version numbers decrease, so v.0.9 is the latest version (not v.1.0). 

To use ATLAS: 
```
ATLAS=/share/apps/genomics/atlas-0.9/atlas
export LD_LIBRARY_PATH=/share/apps/openblas-0.3.6/lib:/share/apps/armadillo-9.100.5/lib64:$LD_LIBRARY_PATH
```

### 04a.0a ATLAS: Find all read groups

We need to split all the bam files by read group. 

First find all the read groups in each population. Use the [04.0a_FindRGs.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/04.0_FindRG.sh)

```
for i in $(ls *mergedReads.bam); do samtools view $i | grep "NC_" | awk -F ":" '{print $1, $2, $3, $4}' |sort |uniq; done

K00124 207 HKWH3BBXX 3
```

Once we have this information, we can make two RG.txt files: 

1. RG.txt which contains the RG for the population
```
export PATH=/share/apps/genomics/samtools-1.9/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/genomics/samtools-1.9/lib:$LD_LIBRARY_PATH

for i in $(ls *realn.bam); do samtools view -H $i |grep "@RG"; done 

nano RG.txt
D3mus  paired
```

2. ALL.RG.txt which contains the RG information for each of the sequencing lanes
```
nano ALL.RG.txt
K00124:207:HKWH3BBXX:3      HKWH3BBXX_lane3      RG1
```


### 04a.0 ATLAS: SplitMerge

**The latest version of ATLAS is 0.9, not 1.0. Ed upgraded to v.0.9 on the UCL shared apps folder so that I can run splitMerge and PMD on the museum samples. 

Create a text file with the ReadGroup names, cycle length (if single end), and paired/single. 



```
ATLAS=/share/apps/genomics/atlas-0.9/atlas
for i in $(ls *realn.bam); do $ATLAS task=splitMerge bam=$i; done
```



### 04a.1b Split bams into RGs

Split all the individual bam files into different RGs

1. Write all of the reads associated with a particular RG to file 

Modify the [04a.1b_WriteRG1_list.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/04a.0b_WriteRG1_list.sh) for each RG for each population. 


2. Split each individual bam file by RG

Modify the [04a.1c_SplitBAMS_RG1.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/04a.0c_SplitBAMS_RG1.sh) script for each RG within each population


3. Calculate the total size of all the bam files for each RG

```
du -sch *RG1*merged*bam
```

If the total bams is <10Gb for a read group, merge the bam files. Use the [MergeBAMS.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/MergeBAMS.sh) script


### 03a.2 ATLAS: PMD - Museum individuals


Estimate the PMD per sample for each of the museum samples. This needs to be run on the full museum samples (with all RGs combined). Don't filter the reads. 

Modify the [04b_ATLAS_MUS.pmd.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/04b_ATLAS_MUS.pmd.sh) script

This will output a pmd correction for each individual.


### 03a.3 ATLAS: recal - All RGs

This should be run on each of the read groups within each of the populations. 

Modify the [04_ATLAS_recal.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/04b_ATLAS_recal.sh)

Files needed to run this: 

pmdFile *Empiric* for the museum bam file

File containing a set of invaraint sites

mergeTheseRGs.txt file with all the RGs within the 

e.g. for E3 museum: 
```
cat mergeTheseRGs.txt 
AH-01-1900-01 AH-01-1900-04 AH-01-1900-05 AH-01-1900-06	AH-01-1900-08	AH-01-1900-09	AH-01-1900-10	AH-01-1900-11	AH-01-1900-13	AH-01-1900-14	AH-01-1900-15	AH-01-1900-16	AH-01-1900-20	AH-01-1900-21	AH-01-1900-22	AH-01-1900-23	AH-01-1900-24	AH-01-1900-25	AH-01-1900-27	AH-01-1900-28	AH-01-1900-29	AH-01-1900-32	AH-01-1900-33	AH-01-1900-34	AH-01-1900-35	AH-01-1900-37	AH-01-1900-38	AH-01-1900-39	AH-01-1900-40	AH-01-1900-41	AH-01-1900-45	AH-01-1900-46	AH-01-1900-47
```


Run recal independently for each sample. In some populations we find that not all the samples run to completion or the run fails to find an appropriate recalibration score. See [here](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/recal_tests.md) for my exploration of these errors. 

Based on the above results (and based on communication with the developers) we will use the median recal score for the samples that did work as the recal score for the samples that failed, given that at least 5 samples produced valid recal scores. 

##### Calculate median recal score

First find all the samples that have run to completion and that do not contain the unconverged/un-initiated values ("1.00000, 0.00000..." see [here](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/recal_tests.md))
```
grep -L "1.00000" *EM.txt >> worked 
for i in $(cat worked); do cat $i >> worked.EM.txt; done

#Remove all header lines
sed -i '/^readGroup/d' worked.EM.txt

#replace all commas with white space
sed -i 's:,:\t:g' worked.EM.txt
sed -i 's: :\t:g' worked.EM.txt

#split the file into fwd and reverse reads and cut the text columns
grep first worked.EM.txt |cut -f4-27 > worked.EM_first.txt
grep second worked.EM.txt |cut -f4-27 > worked.EM_second.txt
```

Calculate the median for all columns and write a median file
```
#open python and import pandas
python
import pandas as pd

##read in files
df1 = pd.read_csv("worked.EM_first.txt", sep="\t")
df2 = pd.read_csv("worked.EM_second.txt", sep="\t")

##Find median (50% in the table)
df1_med = df1.describe()
df2_med = df2.describe()

##write tables
df1_med.to_csv("worked.first.describe", sep=",")
df2_med.to_csv("worked.second.describe", sep=",")
```

Extract the median values and create a median_recalibrationEM.txt file
```
#Extract the median for fwd and reverse
grep "50%" worked.*.describe |cut -d, -f2-27 > worked.median

#Extract text for first three rows
cut -f1-3 worked.EM.txt |head -n 2 > worked.rownames

#paste files together
paste worked.rownames worked.median > median_recalibrationEM_1.txt

#create a colnames file
nano colnames
readGroup	mate	model	quality	position	context

#Add headers to EM file
cat colnames median_recalibrationEM_1.txt > median_recalibrationEM.txt

#replace the comma field separators where expected
sed -i 's/,/ /2;s/,/ /3' median_recalibrationEM.txt

```

Find all the samples that this file will apply to and create their median recal files
```
awk -F "_" '{print $1}' worked > bamlist.worked
diff bamlist bamlist.worked | grep '^<' | sed 's/^<\ //' > samples_with_med_recal

mkdir recalFiles
for i in $(cat samples_with_med_recal); do cp median_recalibrationEM.txt recalFiles/$i"_recalibrationEM.txt"; done
for i in $(cat bamlist.worked); do cp $i*recalibrationEM.txt recalFiles/; done
```



### 10. ATLAS: global diversity

#### 10.1 GLF

Estimate genotype likelihoods for all samples. We do not need to downsample these data to estimate GLFs (as confirmed by developers). We'll downsample later when we're estimating individual diversity (see 11). 

We're estimating GLF separately for modern and museum samples because museum samples need PMD files as well. 

[04c.1_GLF_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/04c.1_GLF_MODC.sh)

[04c.1_GLF_MUS.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/ATLAS/Scripts/04c.1_GLF_MUS.sh)


#### 10.2 MajorMinor


### 11. ATLAS: Individual heterozygosity




#### 12. ATLAS: Output ANGSD input







