# VelocityUCL

NERC Velocity Project analyses using Sanger genomes 

Pipeline set up on UCL servers.

Here I'll curate the variant calling pipeline and analyses undertaken using the Lepidoptera genomes generated by Sanger in 2019/2020.

Darwin Tree of Life (DToL) data - [live results](https://github.com/darwintreeoflife/darwintreeoflife.data/tree/master/species)


## [Skip to pipeline](https://github.com/alexjvr1/VelocityUCL#pipeline)

## [Use the PaleoMix pipeline](https://github.com/alexjvr1/VelocityUCL#paleomix-pipeline)


## General information for working on the UCL server

See [here](https://hpc.cs.ucl.ac.uk/full-guide/) for a general guide of the CS server

For support email: cluster-support [at] ucl.ac.uk

Login
```
#We're biosciences so we're working on the pchuckle node
#staff=tails
#students=knuckles

ssh -l username -J username@tails.cs.ucl.ac.uk pchuckle

```

Script examples
```
/share/apps/examples
```


Shared storage folder - location of all data and shared scripts

```
/SAN/ugi/LepGenomics (3Tb)

#Scripts for Velocity pipeline (you'll be working with the scripts in the pipeline folder)
/SAN/ugi/LepGenomics/VelocityPipeline

#If you add any files to this shared folder please make sure you change permissions so that everyone can access them: 
chmod g+wr filename 
chmod g+wr folder
```

Check space left in shared folder
```
quota -s
```

Copy data to the server from computer

*see TransferData.md for transfer from UoB to UCL server*
```
#1. From your computer
#Port Forwarding with scp (from https://hpc.cs.ucl.ac.uk/ssh-scp/)

If you are trying to copy data to and from the cluster from outside the Computer Science department, it might be necessary to set up a port forward so that you connect to a login node via a jump node.  The command to set this up is

ssh -L 2222:<login node>:22 <username>@<gateway>

For example, if your username was alice and you wanted to connect to the login node called ‘pchuckle’ via the tails gateway server, the command would be:

ssh -L 2222:pchuckle.cs.ucl.ac.uk:22 alice@tails.cs.ucl.ac.uk

Once you have set this up, anything you send to local port 2222 will be forwarded to port 22 on comic via tails (port 22 being the standard port for ssh).

At this point, leave the previous command (ssh -L) running, and open another terminal. You can now scp your data by doing the following:

scp -P 2222 /path/to/file <user name>@localhost:~/path/to/destination

e.g. copy to the shared folder

scp -P 2222 *fastq.gz ajansen@localhost:/SAN/ugi/LepGenomics/C3_Aricia_agestis

This will copy the file data.txt in the user alice’s home directory on their local machine to their home directory on the cluster, via the machine tails.
```

Specifically: 
```
#At destination
#open window1 and activate port forwarding: 

ssh -l ajansen -L 3000:morecambe.cs.ucl.ac.uk:22 ajansen@tails.cs.ucl.ac.uk

#open window2 and rsync files of interest to the local directory:

rsync -auve "ssh -p 3000" $i ajansen@localhost:/SAN/ugi/LepGenomics/C3_Aricia_agestis/04_diploSHIC_simulations_AJvR/job0022*fvec .

```


### Check that files have transferred properly

We can use md5sum to check if files are the same at origin and destination: 

*mac uses md5 instead of md5sum. We can use an alias to use the md5sum commands below: alias md5sum='md5 -r'*
```
#Create md5sum hashes for files at origin

md5sum file1.txt file2.txt file3.txt > hashes

#copy the hashes file to the destination (either cut and paste or cat)
#check that the hashes are the same

md5sum --check hashes
file1.txt: OK
file2.txt: OK
file3.txt: OK

```



## Software required

*Most software can be found in /share/apps OR /share/apps/genomics*

|Software|version|path|
|--------|--------|--------|
|AdapterRemoval|2.3.2|/SAN/ugi/LepGenomics/Software/adapterremoval-2.3.2/build/AdapterRemoval|
|ANGSD|0.935|/share/apps/genomics/angsd-0.935/bin/angsd|
|ATLAS| 0.9 ** latest version|/share/apps/genomics/atlas-0.9/atlas|
|ATLAS| 1.0** older version| /share/apps/genomics/atlas-1.0/atlas|
|bamtools|2.5.1| /share/apps/genomics/bamtools-2.5.1/bin/bamtools|
|BBmap|38.59| /share/apps/genomics/bbmap-38.59/bbmap.sh|
|bcftools|1.9| /share/apps/genomics/bcftools-1.9/bin/bcftools|
|BWA|0.7.17| /share/apps/genomics/bwa-0.7.17/bwa|
|Cutadapt | |/share/apps/python/bin/cutadapt| 
|Cutadapt|2.5|/share/apps/genomics/cutadapt-2.5/bin/cutadapt|
|fastQC|0.11.8| /share/apps/genomics/FastQC-0.11.8/fastqc|
|gatk|3.7.0|/share/apps/genomics/GenomeAnalysisTK-3.7/GenomeAnalysisTK.jar|
|gatk|3.8.1|/share/apps/genomics/GenomeAnalysisTK-3.8.1.0/GenomeAnalysisTK.jar|
|gcc|9.2.0|/share/apps/gcc-9.2.0/bin/gcc|
|GenomeAnalysisTK.jar|3.7| /share/apps/genomics/GenomeAnalysisTK-3.7/GenomeAnalysisTK.jar|
|java|| /share/apps/java/bin/java|
|MapDamage|2.1.1| /share/apps/python-3.8.5-shared/bin/mapDamage **export python and R to PATH. See below**|
|perl ||/share/apps/perl/bin/perl|
|picard |2.20.3|/share/apps/genomics/picard-2.20.3/bin/picard.jar|
|plink2 |v.2|/share/apps/genomics/plink-2.0/bin/plink2|
|python3 |3|/share/apps/python/bin/python3|
|R|4.0.3| /share/apps/R-4.0.3/bin/R|
|samtools|1.9| /share/apps/genomics/samtools-1.9/bin/samtools|
|Trimmomatic|0.39|/SAN/ugi/LepGenomics/Software/Trimmomatic-0.39|
|vcflib|1.0.0| /share/apps/genomics/vcflib-1.0.0/bin|
|vcftools|0.1.13| /share/apps/genomics/vcftools-0.1.13/bin/vcftools|


For some software we have to export the path to the software and their libraries to our PATH: 

ATLAS
```
export LD_LIBRARY_PATH=/share/apps/openblas-0.3.6/lib:/share/apps/armadillo-9.100.5/lib64:$LD_LIBRARY_PATH
```

bwa
```
export PATH=/share/apps/genomics/bwa-0.7.17/bwa:$PATH
```

perl
```
export PATH=/share/apps/perl-5.30.0/bin:$PATH
```

python
```
export PATH=/share/apps/python-3.6.4-shared/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/python-3.6.4-shared/lib:$LD_LIBRARY_PATH

```

java
```
export PATH=/share/apps/java/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/java/lib:$LD_LIBRARY_PATH
```

mapDamage
```
# Software
##python
export PATH=/share/apps/python-3.8.5-shared/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/python-3.8.5-shared/lib:$LD_LIBRARY_PATH

##R
export PATH=/share/apps/R-4.0.3/bin:$PATH

##mapDamage
mapDamage="/share/apps/python-3.8.5-shared/bin/mapDamage"

```


samtools
```
export PATH=/share/apps/genomics/samtools-1.9/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/genomics/samtools-1.9/lib:$LD_LIBRARY_PATH

```




### Note on renaming files

Liverpool raw data is named with digits and a dash before the sample names. e.g. 33-AH-01-1900-47_191121_L001_R2.fastq.gz
The easiest way to rename them is with the Perl rename (note that the native linux rename works the same as mv and is not so useful in this case). 
Create a copy of the [prename.pl](https://gist.github.com/rocarvaj/6abf7dd1e083963a596430ac43f88e34) script in a folder in your home directory called "scripts". 
```
cd
mkdir scripts
nano prename.pl
#paste a copy of the script found at the link above
```

Run the script using perl installed in the shared server folder: 
```
/share/apps/perl-5.30.0/bin/perl5.30.0 ../../prename.pl --help
```


This works with the sed syntax. e.g. this will remove numbers plus dash from the start of the file names: 

Replace USERNAME with your username
```
/share/apps/perl-5.30.0/bin/perl5.30.0 /home/USERNAME/prename.pl 's/^[0-9]+-//' *

#e.g. This will delete all leading numbers and the dash:
/share/apps/perl-5.30.0/bin/perl5.30.0 /home/ajansen/prename.pl 's/^[0-9]+-//' *
```


## Data

See [here](https://github.com/alexjvr1/VelocityUCL/blob/main/RawDataCheck.md) for sample management for Modern3. 




## Concatenate samples before starting pipeline

1. The script expects samples to be named in the same way in both folders. So remove any additional  information from the names. 

2. Use the 00_Concat_Museum1andMuseum2.sh script to generate a submission script for concatenating the samples. 

3. Move all samples that were sequenced only once from museum1 to the 00_raw_reads_museum_FINAL


#### 1. Rename samples

Modern and Museum samples need to be in the format: 

SampleName_R1.fastq.gz and SampleName_R2.fastq.gz

All raw reads contain information about the sequencer and lane that the sample was sequenced in that we need to remove. This is in the format: *_191121_L001* 

Some reads are returned with a leading number and dash: 20-AH-01-1900-09_....fastq.gz. The leading 20- in this case is from the facility and should be removed.
```
#Remove any leading numbers
/share/apps/perl-5.30.0/bin/perl5.30.0 /home/ajansen/prename.pl 's/^[0-9]+-//' *

#Remove sequencer information from reads
#e.g. for our example above
/share/apps/perl-5.30.0/bin/perl5.30.0 /home/ajansen/prename.pl 's/_191121_L001//' *

#For the final dataset samples should be named in the same way in both the museum1 and museum2 folders. 
```


#### 2. Concatenate repeat samples


Modify the script [00_Concat_Museum1andMuseum2.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/00_Concat_Museum1and2.sh). 

Run this in the command line: 
```
#move the script to your species directory

chmod u+x 00_Concat_Museum1andMuseum2.sh
./00_Concat_Museum1andMuseum2.sh
```

This will create a submission script that can be submitted to the server with qsub. 

#check that the request for resources is sensible in the script


#### 3. Move all the samples that were sequenced once to the 00_raw_reads_museum_concat
```
ls 00_raw_reads_museum/ALLSAMPLES/*R1*gz | awk -F "/" '{print $NF}' | awk -F "_" '{print $1}' > museum1.names
ls 00_raw_reads_museum2/ALLSAMPLES/*R1*gz | awk -F "/" '{print $NF}' | awk -F "_" '{print $1}' > museum2.names
ls 00_raw_reads_museum_FINAL/*R1*gz | awk -F "/" '{print $NF}' | awk -F "_" '{print $1}' > concat.names

#Create a list of samples to move from mus1 to the FINAL folder
diff museum1.names museum2.names | grep '1900' | sed 's/^<\ //'> museum1.tomove

#Check that none of these have been concatenated: 
diff concat.names museum1.tomove

count=$(wc -l museum1.tomove | awk '{print $1}')
echo "number of samples to move:" $count

for i in $(cat museum1.tomove); do cp 00_raw_reads_museum/ALLSAMPLES/$i*gz 00_raw_reads_museum_FINAL; done
```



## Tools that look interesting

For detecting parallel evolution - a multivariate approach

[AF-vapeR - Whiting et al. 2021 BioRxiv](https://www.biorxiv.org/content/10.1101/2021.09.17.460770v1.full.pdf)





## Pipeline

### 0. Prepare the data

   00a. [Concatenate resequenced museum data](https://github.com/alexjvr1/VelocityUCL#00a-concatenate-museum-reseq-data) (some individuals have been sequenced >1)

   00b. [FastQC](https://github.com/alexjvr1/VelocityUCL#00b-raw-read-count-and-velocity_samplestats) to count the number of raw reads
  

### 1. Raw to cleaned and processed data

   1a. [Trim adapter sequence using Trimmomatic](https://github.com/alexjvr1/VelocityUCL#1a-adapter-trimming)
        
   1b. [https://github.com/alexjvr1/VelocityUCL#2-adapterremoval](https://github.com/alexjvr1/VelocityUCL#2-adapterremoval)
   
        
#### 2. Map and process

   2a. [Map museum and modern data to Sanger genome](https://github.com/alexjvr1/VelocityUCL#2a-map-to-reference-genome) 
   
   2b. [Process BAM files](README.md#2b-process-bam-files)
   
   2c. [Correct museum data for possible deamination](README.md#2c-mapdamage-run-on-museum-data) (MapDamage -> output = corrected bam file) 
        
   2d. [Downsample modern data to the same depth as the museum data](README.md#2d-downsample-modern-data-to-the-same-coverage-as-in-the-museum-samples)
        
#### 3. [Genetic diversity and Population structure](https://github.com/alexjvr1/Velocity2020#3-angsd)

3.1 ATLAS - Genetic Diversity




3.2 ANGSD - Population Structure

   3.1.a. [ANGSD filters for SFS](README.md#3a-angsd-filters-for-sfs) (ie. no MAF)
        
   3.1.b. [ANGSD filters for population genomics]
        
   3.1.c. [PCAngsd]



#### 4. Variant Calling

   4.1  [Call variants with bcftools mpileup and call]()

   4.2  [Filter SNPs]
   

#### 5. Analysis: Regions under selection (diploSHIC)

   4a. [Between population variables]

   4b. [Within population variables]	
   
   4c. [Demographic model - test and validate]
   
   4d. [Training model]
   
   
#### 6. Analysis: LD 

   6a. [ANGSD estimate LD across the genome]



#### 7. Analysis: 



## DATA: Genome

Aphantopus hyperantus (Ringlet) was the first genome available ([NCBI link](https://www.ncbi.nlm.nih.gov/assembly/GCA_902806685.1)), so the pipeline will be set up with this species. 


## DATA: WGS

Whole genome resequencing data was generated for 38 & 40 modern individuals (sampled 2016-2017 & 2019) from a core and expanding population. Museum data was generated from 48 individuals + resequencing of a subset of individuals to increase read coverage. 



## Pipeline for Velocity project from raw data to mapped reads:

Test your scripts with a smaller subset of the data. This can either be submitted to the server as a script, or you can test the commands in the interactive node

```
qrsh -l tmem=8G, h_vmem=8G
```

If you've submitted a lot of scripts, or the server is busy, you won't get onto the interactive node. 

Previous commands
```
history | grep command-of-interest

#where command of interest = a command you know was in the line you're looking for. eg. if you can't remember the qrsh command: 
history |grep qrsh
```


## NOTE

If there are any R0 reads in your raw_reads folders, these sequences have been trimmed for adapters previously. Please make sure to start with the raw data. 

Ask Alex to replace the data. 

### 0. Prepare Data

#### 00a Concatenate museum reseq data

##### *TIME*

~10 min

##### *METHOD*

If you haven't concatenated the repeat sequenced museum samples yet, do so now: 

A subset of individuals (33 per species) have been sequenced twice to increase mean depth. These raw data need to be concatenated together.

Follow the steps outlined [here](https://github.com/alexjvr1/VelocityUCL/blob/main/README.md#concatenate-samples-before-starting-pipeline)



#### 00b. Raw read count and Velocity_SampleStats

Add species information and raw reads to the Velocity_Samples_Stats shared spreadsheet: 

1. Count the raw reads for each individual from all populations. Update the shared spreadsheet found [here](https://docs.google.com/spreadsheets/d/1q0PjdjiDabJCutWC0NvQog6sbgB7bKcxBNgD8dPc7uQ/edit?usp=sharing). 

Create a FastQC script by modifying the [00_fastqc_raw_museum.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/00_fastqc_raw_museum.sh) script for your species and population. Make the script executable and run it in your input directory (where the raw fastq.gz files are). 

```
chmod u+x 00_fastqc_raw_museum.sh
./00_fastqc_raw_museum.sh

##This will generate a script to submit to the server: 
## $TIMESTAMP will be replaced by the date and time the script was generated
qsub parallel_fastqc.$TIMESTAMP.smsjob.sh
```

Copy all the .html files to your computer and look at the data quality by eye. Raw read counts should be at the top of the page. 


Extract raw read, read length, and GC% information for all the html files: 
```
ls *R1*html | awk -F "_" '{print $1}'  > names
grep "Total " *R1*.html | awk -F "</td>" '{print $8}' | sed 's:<td>::' > file1
grep "Total " *R1*.html | awk -F "</td>" '{print $12}' | sed 's:<td>::' > file2
grep "Total " *R1*.html | awk -F "</td>" '{print $14}' | sed 's:<td>::' > file3
paste names file1 file2 file3 


#Check that R1 and R2 have the same number of reads
#If the final number is the same as the number of samples then R1 and R2 reads matched exactly
ls *html | awk -F "_" '{print $1}'  > names
grep "Total " *.html | awk -F "</td>" '{print $8}' | sed 's:<td>::' > file1
cat file1 |uniq |wc -l 
```


2. Find information about each species' sampling date and site [here](https://docs.google.com/spreadsheets/d/1G9r50W0VV_ANZ19rIvqZpXWFemy2MW76_iXuyBuCQGA/edit?usp=sharing), and update the shared file accordingly. 



### Folder management: 

Reseq data are kept in the following folders:
```
00_raw_data_museum

00_raw_data_museum2

00_raw_reads_museum_FINAL   ## concatenated museum1 and museum2 + all samples that didn't have reseq data added.

01a_Trimmomatic_museum  ## Reads that have been processed with Trimmomatic. 

01b_Adapterremoval_museum  ## Additional adapter removal and merging of overlapping PE reads. I'll point to this folder when mapping. 

02a_museum_mapped  ##see below. This contains all data including concatenated reseq samples. 
```



### 1. Raw to cleaned and processed data

#### Adapter trimming

##### *TIME:*

This runs in 1-2 hours for the full dataset (museum + modern)


##### *METHOD:*

Modern and museum samples arrive demultiplexed by the sequencing facility. 

We're trimming all adapter sequence from the demultiplexed data. We're also removing all sequences that are shorter than 20bp and 5' quality trimmed to remove bases with PHRED quality score of < 20.

We'll use Trimmomatic to remove adapters. We'll use the TrueSeq3 adapters provided with Trimmomatic. ([NEBNext and TruSeq core adapters are the same](https://www.biostars.org/p/349635/))

See [here](https://support.illumina.com/bulletins/2016/12/what-sequences-do-i-use-for-adapter-trimming.html) for commonly used adapters in Illumina projects. 

See the recommendations from the Trimmomatic developers [here](https://www.biostars.org/p/323087/): TruSeq3-PE-2.fa contains the core adapters and some additional adapter sequences that would be in unusual locations. 

Trimmomatic's ILLUMINACLIP options are specifically designed to find sequence "read-through", i.e. where the read length is longer than the sequence (as in 
our museum data). See the manual [here](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf)


##### 1a. Trimmomatic

Create a submission script by modifying the [01a_Trimmomatic.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/VelocityPipeline/pipeline/01a_Trimmomatic.sh) script. 


**NB

Samples have to be renamed before this script is run. The script expects a basename which is the sample name, followed by .R1.fastq.gz or .R2.fastq.gz. 

Use the rename.pl program as described [above](https://github.com/alexjvr1/VelocityUCL#note-on-renaming-files)


Also, ensure that all the fastqc output files are in a different folder. 

```
mkdir fastqc_outfiles
mv *fastqc* fastqc_outfiles
mv *log fastqc_outfiles
```


Run this in the command line to create a submission script:  
```
./01a_Trimmomatic.sh
```



Example Trimmomatic script with the default options in our scripts:
```
java -jar ../Software/Trimmomatic-0.39/trimmomatic-0.39.jar PE -trimlog TrimmomaticTest/trim.log 00_raw_reads_museum_FINAL/ALLSAMPLES/AH-01-1900-02_190312_L008_R1.fastq.gz 00_raw_reads_museum_FINAL/ALLSAMPLES/AH-01-1900-02_190312_L008_R2.fastq.gz -baseout TrimmomaticTest/AH-01-1900-02.trimtest ILLUMINACLIP:../Software/Trimmomatic-0.39/adapters/TruSeq3-PE-2.fa:2:30:8:1:True LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20 MINLEN:20 AVGQUAL:20
```

Once the run is complete we can collect information for the shared data file: 

This will write the sample name and number of read pairs surviving trimming to a file called TrimmomaticStats. 
```
grep "Both Surviving" 01a_Trimmomatic*/*log |awk '{print $1, $4, $7, $8, $20, $21}' >> TrimmomaticStats.log
```



##### 1b. AdapterRemoval

Used for trimming any remaining adapters and merging reads. 


Modify the [01b_AdapterRemover_museum.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/01b_AdapterRemover_museum.sh) script for each population. 

Create the submission scripts: 
```
chmod u+x 01b_AdapterRemover*sh
chmod g+x 01b_AdapterRemover*sh

./01b_AdapterRemover_museum.sh
./01b_AdapterRemover_MODC.sh
./01b_AdapterRemover_MODE.sh

#submit the generated script to queue
```




Here is an example of the script that will be run. 
```
AdapterRemoval=/SAN/ugi/LepGenomics/Software/adapterremoval-2.3.1/build/AdapterRemoval
$AdapterRemoval --collapse --basename $file \
--file1 $file$TAIL1  --file2 $file$TAIL2 --trimns --trimqualities

#For our test: 
pwd
$AdapterRemoval --collapse --basename AH-01-1900-02 --file1 AH-01-1900-02.trimtest_1P.fastq.gz --file2 AH-01-1900-02.trimtest_2P.fastq.gz --trimns --trimqualities
```


All run settings and outputs are written to files called SAMPLENAME.settings. We can extract all the information we're interested from here: 

```
Total number of read pairs: 43667928
Number of unaligned read pairs: 78684
Number of well aligned read pairs: 43589244
Number of discarded mate 1 reads: 0
Number of singleton mate 1 reads: 0
Number of discarded mate 2 reads: 0
Number of singleton mate 2 reads: 0
Number of reads with adapters[1]: 969987
Number of full-length collapsed pairs: 43528725
Number of truncated collapsed pairs: 0
Number of retained reads: 43807131
Number of retained nucleotides: 2145366993
Average length of retained reads: 48.973
```


Extract stats for our shared datasheet: 
```
#Run this in each AdapterRemoval folder
ls *settings |awk -F "." '{print $1}' > names
grep "Total number of read pairs" *settings| awk '{print $6}' > file1
grep "Number of well aligned read pairs" *settings| awk '{print $7}' > file2
grep "Number of reads with adapters" *settings| awk '{print $6}' > file3
grep "full-length collapsed pairs" *settings | awk '{print $6}' > file4
grep "Number of retained reads" *settings| awk '{print $5}' > file5
grep "Average length" *settings| awk '{print $6}' > file6
paste names file1 file2 file3 file4 file5 file6

```

See [here](https://github.com/ewels/MultiQC/issues/1005) for an extensive discussion on how these outputs are calculated. Importantly for us: 

1) Well aligned read pairs: PE sequences that overlap

2) Full-length collapsed pairs: PE sequences that overlap by 11+ bp, and are collapsed into a single read. 

3) retained_reads = R1 + R2 + Singleton R1 + Singleton R2 + Full length Collapsed + Truncated Collapsed








### 2. Map and process

Map museum and modern samples to the Sanger reference genome. Then correct the museum bam files using MapDamage, and downsample the modern data to the same final depth as the corrected museum bam files. 


#### 2a Map to Reference Genome

##### *TIME:*

Museum samples (n=48) ~3 hours 

Modern Core (n=38) ~10 hours 

Modern Exp (n=40) ~10 hours for all but two samples which had to be restarted. They then ran in 4 hours... 

##### *METHOD:*

Make sure you're using the latest version of the reference genome. We're using the references from NCBI RefSeq where available. e.g. [here](https://www.ncbi.nlm.nih.gov/assembly/GCF_905163445.1,GCF_905163445.1/?&utm_source=gquery)

It is more efficient to run this code in the interactive node before submitting the mapping script to queue
```
#Index the reference genome if needed. Check if the *fasta.fai* file exists in the SpeciesName/RefGenome/ folder in your local directory. If not, run the indexing code. 

#index reference genome
BWA=/share/apps/genomics/bwa-0.7.17/bwa
$BWA index RefGenome/*fna


#Create lists of the input names

## museum
##Adapter trimmed and collapsed = 1 file per indiv
cd 01b_AdapterRemoval_museum
ls *collapsed > mus.tomap
##Check that this contains the correct number of samples (48)
wc -l mus.tomap 
#Navigate back to the $SHAREDFOLDER/$SPECIES directory and move the list of sample names there
cd ..
mv 01b_AdapterRemoval_museum/mus.tomap .


## modern
## Adapter trimmed, but not collapsed = 2 files per indiv
ls 01b_AdapterRemoval_MODC/*R1_paired* > modc.names
sed -i 's:01b_AdapterRemoval_MODC/::g' modc.names
sed -i 's:.pair1.truncated::g'  modc.names

#Check that these files list only the modern sample names and not any of the path: 
head modc.names
AH-01-2016-01
AH-01-2016-04
AH-01-2016-05
....


#make output directories. 
mkdir 02a_mapped_museum
mkdir 02a_mapped_MODC
mkdir 02a_mapped_MODE


#If you're running the unmerged pipeline make this folder as well
#The usual pipeline does not use the unmerged Museum data
mkdir 02a_mapped_MUS.unmerged


#Check that you're pointing to the correct reference genome

#Check that the file separator makes sense: 
##sample_name=`echo ${NAME1} | awk -F "_R" '{print $1}'`
#Change the -F "xxx" according to the file names. 
#e.g the above works for files named as follows: 
#HS-01-2016-26_L007_cutadapt_filtered_R2.fastq.gz
#we want only the first part of this name to carry through. 
```

Run the submission scripts: 

[02a_MapwithBWAmem.ARRAY_museum.merged.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02a_MapwithBWAmem.ARRAY_museum.merged.sh)

[02a_MapwithBWAmem.ARRAY_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02a_MapwithBWAmem.ARRAY_MODC.sh)

Modify the MODC script to run it for MODE if you're working with an expanding species. 


##If we don't merge the museum reads, we can use this script to map the PE reads: 

[02a_MapwithBWAmem.ARRAY_museum_forATLAS.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02a_MapwithBWAmem.ARRAY_museum_forATLAS.sh)


Mapped unmerged reads (PE) have the extension "forATLAS.bam", because we can use ATLAS' SplitMerge function to merge these reads later and collect insert size metrics. 


Check that everything has mapped correctly by checking the file sizes. If the mapping is cut short (e.g. by exceeding the requested walltime) the partial bam file will look complete and can be indexed. But the bam file size will be small (~500kb) and empty when you look at it.
```
#To determine file size

du -sh *bam   

#To see bam file
samtools=/share/apps/genomics/samtools-1.9/bin/samtools
$samtools view file.bam | head


#Check the output with samtools flagstat
samtools=/share/apps/genomics/samtools-1.9/bin/samtools
$samtools flagstat file.bam

#make a flagstat log file for all of the samples
for i in $(ls *bam); do ls $i >>flagstat.log && $samtools flagstat $i >> flagstat.log; done
```

Index the bam files with the script [Index.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/Index.sh)

Submit the script in the folder containing the bam files to be indexed. Create a list of bam files (see INPUT in the script), and remember to change all the paths. 




#### 2b. Process BAM files

##### *TIME*

~1 hour per population

##### *METHOD*

We're using Picard Tools to 1) Add Read Group information, 2) Mark Duplicate reads, 3) Perform a local realignment, and 4) validate the final BAM files. 

The links below provide a script for each step. Modify it to run this for each of your populations. 


##### 0) Add Read Group information. 

Formulate the read group name based on the population and the sequencing library

Use the [02b.0_AddRG.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02b.0_AddRG.sh) script to add read groups.

Choose your RGID based on the species and population. e.g. for Ringlet (E3) modern core, RGID=E3modc

Choose your RGLB based on the library the population was sequenced in. e.g. Ringlet modc were sequenced in mod03. RGLB=mod03

Brown Argus modc RGLB=mod02; mode RGLB=mod02, mus RGLB=mus0204

Speckled Wood modc RGLB=mod01; mode RGLB=mod01, mus RGLB=mus0103


##### 1) Mark Duplicate Reads

Sometimes we see duplicate reads in the dataset which originate from the same DNA fragment. We want to filter these out because we assume that all read information is independent. Duplicate reads can arise during library prep as PCR duplicates, or during sequencing when the sequencer sees a single sequencing cluster as two clusters (called optical duplicates). 

Use the [02b.1_MarkDup_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02b.1_MarkDup_MODC.sh) script to remove duplicate reads

##### 2) Local realignment

Local realignment can be useful to optimise mapping to low complexity or repeat regions in the genome. Our mapping tool doesn't do this (although some pipelines like GATK do incorporate local realignment). We will run a local realignment as an independent step. 

Use the [02b.2_LocalRealignment_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02b.2_LocalRealignment_MODC.sh) script for local realignment


##### 3) Validate bam

Finally we'll use Picard Tools to check if our bam/sam files look as expected. 

Use the [02b.3_ValidateSamFile_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02b.3_ValidateSamFile_MODC.sh) script to validate the files. 


See which samples have any errors: 
```
cat *validatesam

```

Common errors that you might get are: 

MISMATCH_MATE_CIGAR_STRING
MISMATCH_MATE_ALIGNMENT_START


See [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035891231-Errors-in-SAM-or-BAM-files-can-be-diagnosed-with-ValidateSamFile) for a summary from the Broad Institute about the possible errors. 


##### 4) Fix any errors, and revalidate 

Generally the FixMateInformation option from picardTools will correct any errors. Modify the [02b.5_FixMate_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02b.5_FixMate_MODC.sh) script. 

Create a list of sample names to be fixed called modc.names.tofix (change the population as needed). The script requires sample names without any tail. e.g.
```
cat modc.names.tofix
AH-01-2016-06
AH-01-2016-12
AH-01-2016-15
AH-01-2016-18
AH-01-2017-21
```

This will produce new bam files called sample.fixed.bam. 

Validate these bam files by modifying the script from step 3 above. 

If there are no more errors, you can delete the old realn.bam files for these samples. And rename the fixed files to realn.bam. 

```
#This will delete the bam and bai files
for i in $(cat modc.names.tofix); do rm $i.realn.ba*; done

mv AH-01-2016-06.fixed.bam AH-01-2016-06.realn.bam
mv AH-01-2016-12.fixed.bam AH-01-2016-06.realn.bam
mv AH-01-2016-15.fixed.bam AH-01-2016-15.realn.bam
...

##Then index all these by modifying the index.sh script from above.
##Make sure all the files are named in the same way. i.e. sample.realn.bam, sample.realn.bai. Some scripts write the indexed file to sample.realn.bam.bai. 
##If this happens, rename them -> mv sample.realn.bam.bai sample.realn.bai
```


##### 5) Get stats with Flagstat

We want to collect metrics to add to our shared spreadsheet. 

Modify the [02b.4_Flagstat_MODC.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02b.4_Flagstat_MODC.sh) script. 

Once all the sample.flagstat files are ready, collect the info for our spreadsheet: 
```
##Modern samples

ls *realn.bam.flagstat |awk -F "." '{print $1}' > names
grep "QC-passed reads" *realn.bam.flagstat |awk '{print $1}' |awk -F ":" '{print $2}' > totalreads
grep "+ 0 mapped" *realn.bam.flagstat |awk '{print $1}' |awk -F ":" '{print $2}' > mapped
grep "+ 0 mapped" *realn.bam.flagstat |awk -F " " '{print $5}' |awk -F "(" '{print $2}' > prop.mapped
grep "properly paired" *realn.bam.flagstat |awk '{print $1}' |awk -F ":" '{print $2}' > pe.mapped
grep "properly paired" *realn.bam.flagstat |awk '{print $6}' |awk -F "(" '{print $2}' > prop.pe.mapped

paste names totalreads mapped prop.mapped pe.mapped prop.pe.mapped


##Museum samples
ls *realn.bam.flagstat |awk -F "." '{print $1}' > names
grep "QC-passed reads" *realn.bam.flagstat |awk '{print $1}' |awk -F ":" '{print $2}' > totalreads
grep "+ 0 mapped" *realn.bam.flagstat |awk '{print $1}' |awk -F ":" '{print $2}' > mapped
grep "+ 0 mapped" *realn.bam.flagstat |awk -F " " '{print $5}' |awk -F "(" '{print $2}' > prop.mapped

paste names totalreads mapped prop.mapped

```



#### 2c. MapDamage run on museum data

##### *TIME*

3-4 hours for 48 samples

##### *METHOD*

[MapDamage2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3694634/) is a package used to estimate and correct for Cytosine deamination (or any other transition/transversion bias in the data). This is a problem anticipated for ancient DNA, and possibly for museum data.

This needs to be locally installed on BlueCrystal. Follow the git install instructions in the tutorial [here](https://ginolhac.github.io/mapDamage/)


Mapdamage is installed on the server as a python library, so the following needs to be added to any script, or run in the interactive node before using mapdamage. 
```
##python
export PATH=/share/apps/python-3.8.5-shared/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/python-3.8.5-shared/lib:$LD_LIBRARY_PATH

##R
export PATH=/share/apps/R-4.0.3/bin:$PATH

##mapDamage
mapDamage="/share/apps/python-3.8.5-shared/bin/mapDamage"

```



1. Create a file listing all the bamfiles

These are the final input files from the previous step. ie. the sample.realn.bam files

```
cd 02a_mapped_museum
ls *realn.bam > bamlist
```

2. Copy the script [02c_MapDamage_forMERGED.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/02c_MapDamage_forMERGED.sh) to the 02a_mapped_museum. Change the job name, the number of threads, and check the path to the reference genome.

Submit to queue.


3. When running on modern samples add the "--stats-only" option


4. Move all the new rescaled bam files:

The rescaled bams are writtedn to a folder for each sample. We want them all on one place: 
```
cd 02c_MapDamage_MUS
mv AH*/*rescaled.bam .
```

5. Index all the bams using the previous indexing script



##### ->>>  To call variants skip to section 3.2


#### 2d. Downsample modern data to the same coverage as in the museum samples

Due to the difference in sample quality between museum and modern samples, mean coverage is much higher for the modern data. This may bias the confidence in variant calls downstream. To avoid this problem I will downsample the modern data to the same mean depth as the museum data.

First filter the bam files to include only reads with PHRED quality >20 and properly paired reads using the [02c_Filter_modern_bam_pp.PHRED20.sh](https://github.com/alexjvr1/Velocity2020/blob/master/02c_Filter_modern_bam_pp.PHRED20.sh) script.

We'll need the names file: 
```
ls 02a_modern_mapped/*bam >> bamfiles.mod.names
sed -i 's:02a_modern_mapped/::' bamfiles.mod.names
```

Use samtools flagstat to calculate the number of properly paired reads in the recalibrated and filtered museum files.

```
module load apps/samtools-1.8
for i in $(ls results*/*flt.bam); do ls $i >> mus.flagstat.log && samtools flagstat $i >> mus.flagstat.log; done
```
Do the same for the modern samples.

Enter these data in the "Rescaled.ProperlyPaired.Q20" column in the Velocity_MapingStatsPerSpecies_AJvR_20190604.xlsx sheet on Dropbox. Calculate the mean number of museum reads and the proportion of modern reads to downsample to.

Use the [02c_Downsample_mod_ARRAY.sh](https://github.com/alexjvr1/Velocity2020/blob/master/02c_Downsample_mod_ARRAY.sh) script to downsample the modern bam files. Remember to change the job name and the PROP variables and create the input file listing all the modern bams.








### 3.1. ANGSD

There seem to be improperly paired reads in my final dataset. To deal with this I need to change the ANGSD flag -only_proper_pairs to 0. 

Also make sure to use the latest commit of ANGSD. 

I'm using: 
```
angsd version: 0.933-18-gfd1a21a (htslib: 1.10.2-61-g8859b09) build(May  6 2020 14:42:05)
```

#### Set up analysis

To speed up the analysis I will split the ANGSD run across the genome; i.e. all indivs will be analysed for regions 0-x in ARRAY1, x-x1 in ARRAY2, etc. For this we need to split the genome up into regions.

Find all the regions (i.e. all chromosomes and contigs) from the reference index file (.fasta.fai):

```
awk '{print $1}' ../RefGenome/*.fna.fai >> regions

cat regions |wc -l
>87

##We'll run these in an ARRAY. 
```

The shortest contig is 11048 and longest is 18856181 for Ringlet

###### *Filters*

I'm starting with bam files, so there are already some filters on the mapping quality of the sequences. Prior to that there are some crude filters during the demultiplexing and trimming steps.


Other possible filters: 

-b[filelist]

-remove_bads 1 : remove reads with 255 flag (not primary, failure and duplicate reads) (1=default)

-uniqueOnly 1 : remove reads with multiple best hits

-minMapQ 20 : PHRED 20. This should already be in place during the mapping.

-minQ 20 : PHRED 20 for individual base score.

-only_proper_pairs 0 : NBNB THIS flag is changed to 0 because some of the reads in my final mus files are not properly paired! 
***OLD FILTER include only properly paired reads (default) and should already have been applied to the museum reads prior to this.  

-trim 0 : We're not trimming any data

-baq 1 : estimate base alignment quality using samtools method. BAQ1 and BAQ2 give me very different final variant counts. BAQ1 is more stringent, but could be removing more loci. BAQ2 has a higher number of false positives. See [here](https://www.biostars.org/p/440490/), and discussion on the ANGSD forum [here](https://github.com/ANGSD/angsd/issues/106) and [here](https://github.com/ANGSD/angsd/issues/97)

###ALLELE FREQUENCY ESTIMATION

-doMajorMinor 4 : Force Major allele based on reference. The minor allele is then inferred using doMajorMinor 1. This option needs to be used when calculating SFS for multiple populations as ANGSD otherwise determines a minor allele within each population. I.e. this may not be the same across all the populations.

-ref [..fasta] : For doMM 4 above we need to specify a reference genome.

-doMaf 10 : calculate minor allele. Estimators are added to gether, so doMaf 10 = doMaf 2 (Known major, unknown minor) + doMaf 8 (directly from counts) 

-doPost 1: calculate posterior probabilities using allele frequencies as prior (or 2 for uniform prior, or 3 for SFS prior)

-SNP_pval 0.05 : Only work with SNPs with a p-value above [float]  ** I'm using 0.05 here because too many loci are removed from the MUS dataset at higher p-values. (see [04a_ANGSD_testFilters.md](https://github.com/alexjvr1/Velocity2020/blob/master/04a_ANGSD_testFilters.md))

-GL 1 : I will estimate genotype likelihoods using the SAMtools model

-minInd 18 : I will remove loci where less than 18 individuals have been genotyped. There are 35-40 indivs per group (MUS, MODC, MODE). This number was chosen to ensure we can find MAF of 2-5% (1/(18*2)=2.8%)   
 
 * I excluded the minInd filter in the final dataset as genotype likelihoods should take this into account. 

-setMinDepth : Discard site if total depth (across all indivs) is below [int]. Use -doCounts to determine the distribution of depths

-setMaxDepth : Discard site if total depth (across all indivs) is above [int]  ##I'll use mean + 2*SD

-setMinDepthInd 2 : Minimum depth for a locus for an individual. This is only applicable for analyses using counts (-doCounts obligatory)

-setMaxDepthInd [int]: I'll use a max of meanDP + 2xSD of depth.

-doCounts 1 : Count of the nucleotide bases per individual

-dumpCounts 2 : write a file with all the allele counts per position per individual

-rmTriallelic 1 : include only biallelic loci

-checkBamHeaders 1 : check that the bam headers are compatable for all files.

#### MaxDepth

To estimate the setMaxDepth filter per population, I first need to write the depth per site in ANGSD. 
Using the prepared SFS script add in the following: 

```
-dumpCounts 1 ##This will write global depth count per site in a .pos file
```

Calculate the mean and SD 
```
pwd
/Users/alexjvr/2020.postdoc/Velocity/E3/ANGSD_FINAL/DepthEstimates/DepthPerBP
MODE.LR761675.1.OCT14.pos.gz
MODC.LR761675.1.OCT14.pos.gz
MUS.LR761675.1.OCT14.pos.gz

#R
MODC.pos <- read.table(gzfile("MODC.LR761675.1.OCT14.pos.gz"), header=T)
MODE.pos <- read.table(gzfile("MODE.LR761675.1.OCT14.pos.gz"), header=T)
MUS.pos <- read.table(gzfile("MUS.LR761675.1.OCT14.pos.gz"), header=T)

mean(MODC.pos$totDepth)+(2*(sd(MODC.pos$totDepth)))
[1] 179.6283
mean(MODE.pos$totDepth)+(2*(sd(MODE.pos$totDepth)))
[1] 293.1328
mean(MUS.pos$totDepth)+(2*(sd(MUS.pos$totDepth)))
[1] 80.07606
```


#### 3a. ANGSD filters for SFS

It's important to include all loci in the SFS. Don't include any filters that'll affect the allele frequencies (e.g. MAF or SNP_pval filters). 

It's also important to polarise the Major/Minor alleles so that they're the same between the populations. 

This is done with -doMajorMinor 4 : Force Major allele based on reference. The minor allele is then inferred using doMajorMinor 1 (i.e. from GL). This option needs to be used when calculating SFS for multiple populations as ANGSD otherwise determines a minor allele within each population. I.e. this may not be the same across all the populations.

The order of the filters do not seem to affect the outcome (see https://github.com/alexjvr1/Velocity2020/blob/master/04a_ANGSD_testFilters.md)

To add depth filters we need to add -doCount 1. 

NBNB for MUS data we have to select only_proper_pairs 0 - this is because the processing with MapDamage changes the reads in some way so that they all get filtered out if we select only_proper_pairs 1. They should already be filtered for proper_pairs based on prior mapping filters. 

The [basic process is](https://github.com/ANGSD/angsd/issues/259): 

###### 1. Estimate SAF for each population (unfolded) 

MUS
```
##Need gcc to be loaded
module load languages/gcc-9.1.0

##ANGSD located here: 
/newhome/aj18951/bin/angsd/angsd

/newhome/aj18951/E3_Aphantopus_hyperantus_2020/04a_ANGSD_TESTS/04_SFS/04a_MUS.SAF.sh

/newhome/aj18951/bin/angsd/angsd -b MUS.poplist -checkBamHeaders 1 -minQ 20 -minMapQ 20 -uniqueOnly 1 -remove_bads 1 -only_proper_pairs 0 -r LR761675.1: -GL 1 -doSaf 1 -anc ../RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna -ref ../RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna -doCounts 1 -setMinDepthInd 2 -setMaxDepth 144 -doMajorMinor 4 -out MUS -C 50 -baq 1 

-> Total number of sites analyzed: 4786628
-> Number of sites retained after filtering: 4762287 
```

MODC
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/04a_ANGSD_TESTS/04_SFS/04a_MODC.SAF.sh

/newhome/aj18951/bin/angsd/angsd -b MODC.poplist -checkBamHeaders 1 -minQ 20 -minMapQ 20 -uniqueOnly 1 -remove_bads 1 -only_proper_pairs 1 -r LR761675.1: -GL 1 -doSaf 1 -anc ../RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna -ref ../RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna -doCounts 1 -setMinDepthInd 2 -setMaxDepth 328 -doMajorMinor 4 -out MODC -C 50 -baq 1 

-> Total number of sites analyzed: 5715589
-> Number of sites retained after filtering: 5621808 

```

MODE
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/04a_ANGSD_TESTS/04_SFS/04a_MODE.SAF.sh

/newhome/aj18951/bin/angsd/angsd -b MODE.poplist -checkBamHeaders 1 -minQ 20 -minMapQ 20 -uniqueOnly 1 -remove_bads 1 -only_proper_pairs 1 -r LR761675.1: -GL 1 -doSaf 1 -anc ../RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna -ref ../RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna -doCounts 1 -setMinDepthInd 2 -setMaxDepth 621 -doMajorMinor 4 -out MODE -C 50 -baq 1 

-> Total number of sites analyzed: 5688111
-> Number of sites retained after filtering: 5553623 
```


###### 2. unfolded SAF used to produce folded 2D SFS

Then generate the folded SFS for each population pair per chromosome. If the dataset is smaller this can be run on a merged dataset (realSFS cat xx -fold 1 -outnames pop.saf.out), but this is too memory intensive for the whole genome datasets. 
```
realSFS pop1.unfolded.saf.idx pop2.unfolded.saf.idx -fold 1 >folded.sfs

~/bin/angsd/misc/realSFS MODC.saf.idx MODE.saf.idx -fold 1 > MODE.MODC.fold.sfs

~/bin/angsd/misc/realSFS MODC.saf.idx MUS.saf.idx -fold 1 > MUS.MODC.fold.sfs
```

##### 3. unfolded SAF and folded SFS used to generate per-site numerator/denominator of Fst (use -fold in realSFS fst index)
```
realSFS fst index pop1.unfolded.saf.idx pop2.unfolded.saf.idx -sfs folded.sfs -fold 1 -fstout persite

~/bin/angsd/misc/realSFS fst index MODC.saf.idx MUS.saf.idx -sfs MUS.MODC.folded.sfs -fstout MUS.MODC.fstout
~/bin/angsd/misc/realSFS fst index MODC.saf.idx MODE.saf.idx -sfs MODE.MODC.folded.sfs -fstout MODE.MODC.fstout
```

##### 4. sum numerator and denominator in windows
```
realSFS fst stat2 persite.fst.idx -win XXXX -step XXXX >window.fst

~/bin/angsd/misc/realSFS fst stats2 here.MUS.MODC.fst.idx -win 50000 -step 10000 > slidingwindow.MUS.MODC
~/bin/angsd/misc/realSFS fst stats2 MODE.MODC.persite.fst.idx -win 50000 -step 10000 > MODE.MODC.slidingwindow
```


##### GlobalFST 

1. MODC vs MODE
```
~/bin/angsd/misc/realSFS fst stats MODE.MODC.persite.fst.idx

-> FST.Unweight[nObs:5498007]:0.020322 Fst.Weight:0.126046
0.020322	0.126046
```

2. MODC vs MUS
```
~/bin/angsd/misc/realSFS fst stats MUS.MODC.fstout.fst.idx 
	-> Assuming idxname:MUS.MODC.fstout.fst.idx
	-> Assuming .fst.gz file: MUS.MODC.fstout.fst.gz
	-> FST.Unweight[nObs:4738713]:0.091403 Fst.Weight:0.234494
0.091403	0.234494
```


##### Diversity stats with ANGSD (theta)

```
##MODE
~/bin/angsd/misc/realSFS MODE.saf.idx -fold 1 > MODE.fold.sfs
~/bin/angsd/misc/realSFS saf2theta MODE.saf.idx -sfs MODE.fold.sfs -outname MODE.fold.thetas
 
##MODC
~/bin/angsd/misc/realSFS MODC.saf.idx -fold 1 > MODC.fold.sfs
~/bin/angsd/misc/realSFS saf2theta MODC.saf.idx -sfs MODC.fold.sfs -outname MODC.fold.thetas

##MUS
~/bin/angsd/misc/realSFS MUS.saf.idx -fold 1 > MUS.fold.sfs
~/bin/angsd/misc/realSFS saf2theta MUS.saf.idx -sfs MUS.fold.sfs -outname MUS.fold.thetas

```


##### Depth vs Fst and Depth vs nucleotide diversity

I'm concerned that the sequencing depth of the samples might affect the final Fst and nucleotide diversity estimates. This is particularly problematic given the difference in sequencing depth between museum and modern samples. Here I plot depth vs Fst and depth vs nucleotide diversity for each of the datasets. 

Nucleotide diversity and Fst have been estimated above. 

###### 1. Obtain depth estimates: 

ANGSD estimates depth using the -doCounts 1 -doDepth 1 -dumpCounts 2

.counts.gz = depth per individual per site. Columns = indivs, Rows = sites
.pos.gz = totalDepth per site across all indivs. 


See [03_DepthEstimate.md](https://github.com/alexjvr1/Velocity2020/blob/master/03_DepthEstimate.md) for my estimates of depth across the different datasets and how I chose my minDP and maxDP filters. 



#### 2. Call GL

Test dataset: 
GL are called using the samtools method (GL1), and we're only allowing SNPs with a p-value of 0.001 for modern samples, and 0.05 for museum samples. 


Total sites analyzed and kept for each dataset: 

MOD Core p0.001 per SNP
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/03a_ANGSD_SFS/logfiles

grep "Total number of sites analyzed" E3_MODC.ANGSD1.ARRAY.o9325111-* | awk -F "analyzed:" '{s+=$2} END {print s}'
381121498

grep "retained" E3_MODC.ANGSD1.ARRAY.o9325111-* | awk -F "filtering:" '{s+=$2} END {print s}'
6645193
```

MOD Exp p0.001 per SNP
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/03a_ANGSD_SFS/logfiles

grep "Total number of sites analyzed" E3_MODE.ANGSD1.ARRAY.o9325110-* | awk -F "analyzed:" '{s+=$2} END {print s}'
379718544

grep "retained" E3_MODE.ANGSD1.ARRAY.o9325110-* | awk -F "filtering:" '{s+=$2} END {print s}'
5148017
```

*p0.05 per SNP*

MOD Core p0.05 per SNP
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/03a_ANGSD_SFS/logfiles

grep "Total number of sites analyzed" E3_MODC.ANGSD1.ARRAY.o9541530-* | awk -F "analyzed:" '{s+=$2} END {print s}'
381121498 ##OLD p0.001
381142712

grep "retained" E3_MODC.ANGSD1.ARRAY.o9541530-* | awk -F "filtering:" '{s+=$2} END {print s}'
6645193 ##OLD p0.001
7529770
```

MOD Exp p0.05 per SNP
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/03a_ANGSD_SFS/logfiles

grep "Total number of sites analyzed" E3_MODE.ANGSD1.ARRAY.o9541531-* | awk -F "analyzed:" '{s+=$2} END {print s}'
379718544 ##old p0.001
379785246

grep "retained" E3_MODE.ANGSD1.ARRAY.o9541531-* |awk -F "filtering:" '{s+=$2} END {print s}'
5148017 ##old p0.001
6265127
```


MUS
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/03a_ANGSD_SFS/logfiles

grep "Total number of sites analyzed" E3_ANGSD1.ARRAY.o9403374* | awk -F "analyzed:" '{s+=$2} END {print s}'
346540624

grep "retained" E3_ANGSD1.ARRAY.o9403374* | awk -F "filtering:" '{s+=$2} END {print s}'
23542320
```



ANGSD is performed across all indivs for each contig/scaffold seperately. I then need to merge all of the data into a single file. 

```
module load languages/gcc-6.1

~/bin/angsd/misc/realSFS cat BA.HOD.*idx -outnames MUS.MERGED
```

#### 2d. ISSUES

##### *1. Input bam files*

I've had problems with reading merged bam files created by BBmerge into R. I've reported the issue [here](https://github.com/ANGSD/angsd/issues/260) - it seems to be the same problem encountered by someone using inputs using miniMap2

```
[bammer_main] 1 samples in 1 input files
-> Parsing 1 number of samples
No data for chromoId=0 chromoname=LR761647.1
This could either indicate that there really is no data for this chromosome
Or it could be problem with this program regSize=0 notDone=0

-> Done reading data waiting for calculations to finish
-> Done waiting for threads
-> Output filenames:
	->"angsdput.arg"
-> Fri Apr 17 16:45:14 2020

-> Arguments and parameters for all analysis are located in .arg file
-> Total number of sites analyzed: 0
-> Number of sites retained after filtering: 0 
[ALL done] cpu-time used =  4.24 sec
[ALL done] walltime used =  4.00 sec
```

The developers have fixed a bug in the program. In addition I need to change the -only_proper_pairs flag to 0 as there are improperly paired reads in the merged file (i.e. reads located on different scaffolds). 


#### 3. Compare downsampled data to full dataset for modern pops (SFS and GL)


### 3.2. CALL VARIANTS

Call SNPs for one chromosome using bcftools mpileup and call


1. Index the reference genome

This creates a .fai file that contains chromosome name, length, start and stop positions. 
```
module load apps/samtools-1.9.1
samtools faidx RefGenome/*fna
```

2. Create a folder for calling variants for each population (modern.exp and modern.core are called together)
```
pwd
$HOME/$SPECIES/

mkdir 03.2_Variant_calling_modern
mkdir 03.2_Variant_calling_museum
```

3. Copy the variant calling script to each folder

Script here: [03a_variant_calling_bluecp3.sh](https://github.com/alexjvr1/Velocity2020/blob/master/03a_variant_calling_UCL.sh)

a) Edit the variables in the script for each species and population

b) Change permissions to execute this in your home directory. This should change the colour of the script. 
```
chmod u+x 03a_variant_calling_bluecp3.sh
```

c) And run 

```
./03a_variant_calling_bluecp3.sh
```

This  will create a regions file with all the chromosomes and scaffolds (obtained from the .fai indexed reference file), and a submission script that we'll submit to the server (*.sh)

d) Check the length of this file. 

Delete all scaffolds (we're only interested in the chromosomes for now). 

And add a header to the file. 


e) Change the number of threads in the variant calling script to match the number of chromosomes. 

This script calls variants for all individuals simultaneously per chromosome. 

f) submit to queue and check that it's running properly

```
qstat -u username -t
```



### 4. ANALYSES

Example papers

[Crested Ibis (Feng et al, Current biology, 2019)](https://www.sciencedirect.com/science/article/pii/S0960982218316099)

[Alpine chipmunks (Bi et al, PLoS Genetics, 2019)](https://journals.plos.org/plosgenetics/article?rev=2&id=10.1371/journal.pgen.1008119)

- Also good example of mapdamage use

- PopGen

- outFlank

[Dryas Monkey (ven der Valk, MBE, 2020)](https://academic.oup.com/mbe/article/37/1/183/5570178)


[Darwins finches (Pachecho, GBE, 2020)](https://academic.oup.com/gbe/article/12/3/136/5735467)

- example of using ANGSD to call GL

- diversity stats

[Lynx (Lucena-Perez, MolEcol, 2020)](https://onlinelibrary.wiley.com/doi/full/10.1111/mec.15366?casa_token=I6nUZVYLpjAAAAAA%3AjGkueGUyt5AP-RdrD2GeejFl-U2BXVlrXqmYLcck4lOfFy4Yb2jMv3r6ZxXZ9WhGYDcmVDM0oJ8l)

- Genotype calls

- Pop structure

- Diversity measures

[Killer whale ROH (BioRxiv, Andy Foote & Excoffier)](https://www.biorxiv.org/content/10.1101/2020.04.08.031344v1.abstract)

- ROH


#### 4a. Diversity stats

##### 1. Tajima's D and Watterson's theta

We can calculate Tajima's D and Watterson's theta as [diversity stats in ANGSD](http://www.popgen.dk/angsd/index.php/Thetas,Tajima,Neutrality_tests) after estimating the SFS for each population. When using folded SFS (as we have here) only the TD and WT will be meaningful despite several stats being calculated by these scripts: 

```
##1. Calculate the SFS from SAF. -fold 1 for folded SFS when reference is unknown

~/bin/angsd/misc/realSFS NAME.saf.idx -fold 1 > NAME.saf.sfs

## or use the script below and input names in the command line. Where $1 = input saf, and $2= output file name: 

qsub 04a_ANGSD_realSFS_cmdlineInputs.sh -F "NAME.saf.idx NAME.saf.sfs"

##2. Estimate thetas from SFS

~/bin/angsd/misc/realSFS saf2theta NAME.saf.idx -sfs NAME.saf.sfs -outname NAME.THETA.theta.gz

##3. Estimate thetas in windows from the above global theta file

~/bin/angsd/misc/thetaStat do_stat NAME.theta.idx -win 50000 -step 10000 -outnames NAME.THETA.window.gz

#4. Estimate theta per chromosome

~/bin/angsd/misc/thetaStat do_stat NAME.theta.idx
```


###### Scripts: 

Step1: [04a_ANGSD_realSFS_cmdlineInputs.sh](https://github.com/alexjvr1/Velocity2020/blob/master/04a_ANGSD_realSFS_cmdlineInputs.sh)


##### 2. observed heterozygosity

Comparison of the number of heterozygous sites in historic vs current datasets

Calculate this in ANGSD

###### 1. Estimate the SFS for each individual separately by modifying the script [04a_ANGSD_INDIV.SAF.sh](https://github.com/alexjvr1/Velocity2020/blob/master/04a_ANGSD_INDIV.SAF.sh) to first estimate SAF

###### 2. Estimate individual SFS from SAF 

Create a list of all the idx files for each population
```
cd HET.per.INDIV
ls MUS*idx >> MUS.idx.names
ls MODE*idx >> MODE.idx.names
ls MODC*idx >> MODC.idx.names
ls MUS*baq2*idx >> MUS.idx.baq2.names  ##I compared the MUS datasets with the two BAQ settings
```

Modify the script [04a_ANGSD_SFS.INDIV.sh](https://github.com/alexjvr1/Velocity2020/blob/master/04a_ANGSD_SFS.INDIV.sh) and run in the same folder for each population


###### 3. Concat all files together and read into R and plot
```
/newhome/aj18951/E3_Aphantopus_hyperantus_2020/04a_ANGSD_TESTS

cat MUS*sfs >> MUS.ALL.sfs
cat MODC*sfs >> MODC.ALL.sfs
cat MODE*sfs >> MODE.ALL.sfs
cat MUS*baq2*sfs >> MUS.ALL.baq2.sfs

##copy to mac
scp bluecp3:/newhome/aj18951/E3*/04*/HET.per.INDIV/*ALL.sfs .


#in R
a<-read.table("MODC.ALL.sfs", header=F)
b<-read.table("MODE.ALL.sfs", header=F)
c<- read.table("MUS.ALL.sfs", header=F)
d<-read.table("MUS.ALL.baq2.sfs", header=F)
e<-read.table("MODC.ALL.baq2.sfs", header=F)
f<-read.table("MODE.ALL.baq2.sfs", header=F)

colnames(a) <- c("Hom1", "Het", "Hom2")
a$pop <- "MODC"
colnames(b) <- c("Hom1", "Het", "Hom2")
b$pop <- "MODE"
colnames(c) <- c("Hom1", "Het", "Hom2")
c$pop <- "MUS"
colnames(d) <- c("Hom1", "Het", "Hom2")
d$pop <- "MUS.baq2"
colnames(e) <- c("Hom1", "Het", "Hom2")
e$pop <- "MODC.baq2"
colnames(f) <- c("Hom1", "Het", "Hom2")
f$pop <- "MODE.baq2"

abc <- rbind(a,b,c,d,e,f)
abc$HetFreq <- abc$Het/(abc$Hom1+abc$Het+abc$Hom2)

library(ggplot2)
ggplot(abc, aes(pop, HetFreq, colour=pop)) + geom_boxplot()

Find the heterozygosity for each individual and paste into Table:Ringlet_DiversityStats.xlsx on Dropbox

```



##### 2b. Depth vs Fst

Estimate SFS for all three population pairs. And then 

Pop1=MODC

Pop2=MUS

Pop3=MODE


04a_ANGSD_FINAL/SFS_and_Fst
```
module load languages/gcc-6.1
~/bin/angsd/misc/realSFS fst index MUS/MUS.LR761675.1.saf.idx MODE/MODE.LR761675.1.saf.idx -sfs MUS.MODE.LR75.test.fold.sfs -fstout MUS.MODE.fstout
~/bin/angsd/misc/realSFS fst stats  MUS.MODE.fstout.fst.idx 

	-> Assuming idxname:MUS.MODE.fstout.fst.idx
	-> Assuming .fst.gz file: MUS.MODE.fstout.fst.gz
	-> FST.Unweight[nObs:4722251]:0.017379 Fst.Weight:0.314412
0.017379	0.314412



~/bin/angsd/misc/realSFS fst index MODC/MODC.LR761675.1.saf.idx MUS/MUS.LR761675.1.saf.idx MODE/MODE.LR761675.1.saf.idx -sfs MODC.MUS.LR75.test.fold.sfs -sfs MODC.MODE.LR75.test.fold.sfs -sfs MUS.MODE.LR75.test.fold.sfs -fstout 3pops.fstout

~/bin/angsd/misc/realSFS fst stats 3pops.fstout.fst.idx

~/bin/angsd/misc/realSFS fst stats2 3pops.fstout.fst.idx -win 50000 -step 10000 > 3pops.slidingwindow.fst
```
The above script provides a window-based estimate of Fst across each chromosome. 



The depth estimates need to be obtained using samtools flagstat on the original bamfiles. This script writes a depth estimate for each individual for each site
```
#!/bin/bash
###########################################
# (c) Alexandra Jansen van Rensburg
# last modified 12/07/2019 05:49 
###########################################

## Index all bamfiles listed in bamlist

#PBS -N E1.index  ##job name
#PBS -l nodes=1:ppn=1  #nr of nodes and processors per node
#PBS -l mem=16gb #RAM
#PBS -l walltime=10:00:00 ##wall time.
#PBS -j oe  #concatenates error and output files (with prefix job1)
#PBS -t 1-48

#run job in working directory
cd $PBS_O_WORKDIR


#load modules

module load apps/samtools-1.8

##Set up array

NAME=$(sed "${PBS_ARRAYID}q;d" bamlist)

##Run script
echo "Output depth stats for ${NAME}"
printf "\n"

echo "time samtools depth ${NAME} > ${NAME}.depth" 
time samtools depth ${NAME} > ${NAME}.depth
```

Here I'm extracting chromosome LR..75 and estimating individual depth: 
```
#!/bin/bash
###########################################
# (c) Alexandra Jansen van Rensburg
# last modified 12/07/2019 05:49 
###########################################

## Index all bamfiles listed in bamlist

#PBS -N LR75  ##job name
#PBS -l nodes=1:ppn=1  #nr of nodes and processors per node
#PBS -l mem=16gb #RAM
#PBS -l walltime=10:00:00 ##wall time.
#PBS -j oe  #concatenates error and output files (with prefix job1)
#PBS -t 1-48

#run job in working directory
cd $PBS_O_WORKDIR


#load modules

module load apps/samtools-1.8

##Set up array

NAME=$(sed "${PBS_ARRAYID}q;d" bamlist)

##Run script
## && ensures line is complete before starting next command. 

echo "Output depth stats for ${NAME}"
printf "\n"

echo "time samtools view -b ${NAME} LR761675.1 > ${NAME}.LR75.bam" &&\
time samtools view -b ${NAME} LR761675.1 > ${NAME}.LR75.bam && \
echo "time samtools index ${NAME}.LR75.bam" && \
time samtools index ${NAME}.LR75.bam && \
echo "time samtools depth ${NAME}.LR75.bam > ${NAME}.LR75.depth" &&\
time samtools depth ${NAME} > ${NAME}.LR75.depth
```

For the modern sampels I'm using the *flt.bam* files as they have been filtered for > PHRED 20 quality scores. 


We want to plot variance in depth per site vs Fst and nucleotide diversity. So we need to join each of the depth files together within each pop (e.g. a MUS depth file), where each column is an individual, and each row is a site. At the same time we need to add missing data or gaps where loci do not co-occur between populations. This can all be done in R: 

Multi-merge script from [here](https://www.r-bloggers.com/merging-multiple-data-files-into-one-data-frame/)
Modified to read tables and to merge (all=T) by including NA for all the missing data (otherwise only the intersecting data will end up in the final file). 
```
multmerge = function(mypath){
filenames=list.files(path=mypath, full.names=TRUE)
datalist = lapply(filenames, function(x){read.table(file=x,header=T)})
Reduce(function(x,y) {merge(x,y) all=T}, datalist)

#After running the code to define the function, you are all set to use it. The function takes a path. This path should be the name of a folder #that contains all of the files you would like to read and merge together and only those files you would like to merge. With this in mind, I #have two tips:

#1. Before you use this function, my suggestion is to create a new folder in a short directory (for example, the path for this folder could be #“C://R//mergeme“) and save all of the files you would like to merge in that folder.

#2. In addition, make sure that the column that will do the matching is formatted the same way (and has the same name) in each of the files.
```




##### 3. nucleotide diversity in windows

ThetaD from ANGSD window-based approach

Fig 3a in Feng et al: Distribution of nucleotide diversity across chromosomes in old vs new in 5Mb windows using nuc.div function in pegas


##### 4. Runs of Homozygosity 

Runs of homozygosity: Dryas monkey MS


Convert output to plink genotype calls: http://www.popgen.dk/angsd/index.php/Plink

Only use indivs with mean depth >> 3x? 

Use Plink to look for ROH in sliding window blocks of 50SNPs with one SNP per xkb (50kb?)
"allow for a maximum of one heterozygous and five missing calls per window before we considered the ROH to be broken"



##### 5. IBD in windows across chromosomes

Fig 3b in Feng et al: Distribution of IBD across chromosomes

##### 6. Deleterious load

Deleterious load: See Feng et al. 2019 method



#### 4b. Population structure

NJ phylogeny (Feng et al)

PCA [using ANGSD](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.5231). 

Use [PCAngsd](http://www.popgen.dk/software/index.php/PCAngsd) for this. We need Beagle output from ANGSD

For this we want to estimate genotypes (-GL 1) with confidence (-SNP_pval xx) with Beagle output (-doGlf 2)

Basic options used: 
```
time $angsd -b $POPLIST -checkBamHeaders 1 -minQ 20 -minMapQ 20 -uniqueOnly 1 -remove_bads 1 \
-only_proper_pairs 1 -r ${REGION} -GL 1 -doGlf 2 -out PCAngsd/GL.ALL.${REGION}\
-doSaf 1 -ref ../RefGenome/*fna -anc ../RefGenome/*fna -rmTriallelic 1 \
-doCounts 1 -dumpCounts 2 -doMajorMinor 4 -doMaf 1 -skipTriallelic 1 \
 -setMinDepthInd $minDP setMaxDepth $MAXDP -baq 1 -C 50 \
 -SNP_pval 0.001 -doGeno 32 -doPost 1


#Where:
POPLIST = all of the bam files for all of the populations
REGION = chromosome as an array script
```

I initially called genotypes for all 3 populations together, but as the GLs are then based on all bam files, I ended up with equal likelihoods for all three genotypes for the majority of the loci. This approach is also fundamentally flawed as the likelihood of the museum data cannot be based on the modern data. 

My new approach is to estimate the GL files for each of the populations separately and then combine the GL files for the loci found in all three datasets. 

I will try this with different filter sets. The -doMM 1 and -doMaf 1 should be the strictest settings. 

1. p-val 0.01, doMajorMinor 4, doMaf 2

2. p-val 0.05; -doMM 1; -doMaf 2

3. p-val 0.05; -doMM 4; -doMaf 1

4. p-val 0.05; -doMM 1; -doMaf 1

5. p-val 0.01; -doMM 1; -doMaf 1


-doMaf 1 vs 2 

-doMM 1 vs 4

Before concatenating these files I checked the proportion of equal likelihood genotypes in each dataset. This seemed unaffected by the filter set, and was high for all three datasets. 

These are results for LR7616175.1

MODC: ~50k loci; 38% equal likelihoods

MODE: ~57k loci; 29% equal likelihoods

MUS: ~5.5k loci; 58% equal likelihoods

```
## Count the number of loci. LR for chromosomes CADX for contigs
grep LR *beagle |wc -l 

## Check the number of indivs in the beagle file by looking at the top of the file

## Total number of GLs = #loci x #indivs x 3

## Count number of occurrences of equal likelihood

grep -i -o "0.333" *beagle |wc -l

##frequency = #occurrences/Total GLs
```

##### Filter Beagle GLs

https://faculty.washington.edu/browning/beagle_utilities/utilities.html

If a genotype has less than xx likelihood it's marked as missing data. But how does this then affect the PCA? 



#### 4c. Outliers, Map, and identify candidates in the area. 

PCAngsd as outlier approach?? Does this make sense? 
```


```



##### Fst in windows to find outlying regions in ANGSD

ANGSD generates a ML SFS from genotype likelihoods for each population. It then uses the SFS as a prior in an empirical Bayes approach to estimate the posterior probability for all possible allelic frequencies at each site. (Method: Fumagalli et al. 2013) 
```

```

Plot Time vs space Fst plots for each chromosome. I'm using the Ringlet chromosome names as in the genome MS. Each chromosome is shown in two plots - plot1=MODC vs MODE, plot2= MODC vs Mus. 

Modern graphs
```
p1 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761647.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("Mod Core vs Mod Expanding LR761647.1") + theme(axis.title.x = element_blank())

p2 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761648.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("Mod Core vs Mod Expanding LR761648.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p3 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761649.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761649.1") + xlim(0,18830000) + theme(axis.title.x = element_blank())

p4 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761650.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761650.1") + xlim(0,18830000) + theme(axis.title.x = element_blank())

p5 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761651.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761651.1") + xlim(0,18830000) + theme(axis.title.x = element_blank())


p6 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761652.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761652.1") + xlim(0,18830000) + theme(axis.title.x = element_blank())


p7 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761653.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761653.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p8 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761654.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761654.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p9 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761655.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761655.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p10 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761656.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761656.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())


p11 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761657.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761657.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p12 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761658.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761658.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p13 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761659.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761659.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p14 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761660.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761660.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p15 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761661.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761661.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())


p16 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761662.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761662.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())


p17 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761663.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761663.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p18 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761664.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761664.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p19 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761665.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761665.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p20 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761666.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761666.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())


p21 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761667.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761667.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p22 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761668.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761668.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p23 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761669.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761669.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p24 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761670.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761670.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p25 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761671.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761671.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())


p26 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761672.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761672.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())


p27 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761673.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761673.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p28 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761674.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761674.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

p29 <- ggplot(fstMODC.MODE[which(fstMODC.MODE$chr=="LR761675.1"),], aes(x=midpoint, y=fst)) + geom_point() + ggtitle("LR761675.1") + xlim(0,18830000)+ theme(axis.title.x = element_blank())

```




###### Synteny between modern and museum samples. 

BLAST with FlyBase & Enrichment analysis using PANTHER  

See [this](https://onlinelibrary.wiley.com/doi/full/10.1111/mec.15188?casa_token=X-WHMot7TDcAAAAA%3Abn7IwwiinA44JDoEU-yuVV3iLk4RkXwcCU1av3_hKRG1hgKDNaCzPHbrEGlRCBk5j8bMcIW6ynjT) example paper



#### 4d. LD analyses

ngsLD (https://github.com/fgvieira/ngsLD) and plot r2 estimates using fit_LDdecay.R

Can use GL as input

See pigeon paper 


#### 4e. 



## Old CODE

####### CS at UCL

If you're running on the CS servers at UCL, skip to the next section ["Generate submission script"](https://github.com/alexjvr1/VelocityUCL#generate-submission-script). 


####### BlueCrystal (UoB users) 

If you're running this on BlueCrystal (UoB), you'll have to install cutadapt locally first using [these instructions](https://cutadapt.readthedocs.io/en/stable/installation.html) - see below. 

We're using cutadapt version 3.4: 
```
module load languages/python-anaconda3-5.2.0

module load tools/cmake-3.8.1
module load tools/autoconf-2.69
module load languages/gcc-9.1.0
module add tools/nasm-2.15.05

#install cutadapt in your home directory using the web instructions
pip3 install --user --upgrade cutadapt

#Check that this cutadapt works
~/.local/bin/cutadapt --help


##Check if this directory is in your PATH:
echo $PATH

##And add to PATH if it isn't yet
PATH="$PATH:~/.local/bin/"

##Now you can run cutadapt directly
cutadapt --help

cutadapt version 3.4
```

##### Generate submission script

Run the following scripts to generate the submission script. Be sure to modify the path for your species, and to point to the correct input and output folders depending on the population you're working with. 

You'll need to run this script for each population (MODC, MODE, MUS).

Also create and check permissions for each output folder.
```
mkdir 01a_museum_cutadapt_reads
mkdir 01a_modern_cutadapt_reads
mkdir 01a_modern_exp_cutadapt_reads
```

```
#Check permissions

ls -ltr folder.name

#Change permissions
# r=read
# w=write
# x=execute
#You don't need all three for all files. e.g. data files can't be executed (bam, fastq, etc), so just use chmod u+rw. 

## for the user
chmod u+rwx 

## for the group
chmod g+rwx
```

[01a_museum_cutadapt_filtering_trimming.sh](https://github.com/alexjvr1/VelocityUCL/blob/main/Scripts/01a_museum_cutadapt_filtering_trimming.sh)


Run this locally
```
./01a_museum_cutadapt_filtering_trimming.sh
```

This will generate a script in the folder you intend to use as your output folder: 

```
01a_parallel_cutadapt_bluecp3.sh
```

Edit this submission script to submit from your home directory:

```
1. Set all paths to your home directory if necessary. 

2. Adjust the number of threads (PBS -t 1-xx) to equal the number of individuals to be analysed. 

3. Check that any empty arguments have been removed from the cutadapt command

4. You might have to set the path to cutadapt to find your local version

```






## PaleoMix Pipeline

- See manual [here](https://paleomix.readthedocs.io/en/stable/bam_pipeline/usage.html)

- Install PaleoMix for the local user on CS

```
#PaleoMix is a Python3 package so can be installed using pip
#Several versions of Python is installed on the CS server. Log onto your home directory and point to the python you want to use: 

source /share/apps/source_files/python/python-3.6.4.source

#Install PaleoMix locally. Make sure we're all using the same version. 
python3 -m pip install paleomix==1.3.5

#Check the installation
paleomix

Version: 1.3.5

Pipelines:
    paleomix bam              -- Pipeline for trimming and mapping of NGS reads.
    paleomix trim             -- Equivalent to the 'bam' pipeline, but only runs
                                 the FASTQ trimming steps.
    paleomix phylo            -- Pipeline for genotyping and phylogenetic
                                 inference from BAMs.
    paleomix zonkey           -- Pipeline for detecting F1 (equine) hybrids.

BAM/SAM tools:
    paleomix coverage         -- Calculate coverage across reference sequences
                                 or regions of interest.
    paleomix depths           -- Calculate depth histograms across reference
                                 sequences or regions of interest.
    paleomix rmdup_collapsed  -- Filters PCR duplicates for collapsed paired-
                                 ended reads generated by the AdapterRemoval
                                 tool.

VCF/GTF/BED/Pileup tools:
    paleomix vcf_filter       -- Quality filters for VCF records, similar to
                                 'vcfutils.pl varFilter'.
    paleomix vcf_to_fasta     -- Create most likely FASTA sequence from tabix-
                                 indexed VCF file.

If you make use of PALEOMIX in your work, please cite
  Schubert et al, "Characterization of ancient and modern genomes by SNP
  detection and phylogenomic and metagenomic analysis using PALEOMIX".
  Nature Protocols. 2014 May; 9(5): 1056-82. doi: 10.1038/nprot.2014.063

```

Paleomix is located here: 
```
paleomix=~/.local/bin/paleomix
```

To use this on the computing node (qrsh) or in a script, the following is needed: 
```
source /share/apps/source_files/python/python-3.6.4.source

paleomix=~/.local/bin/paleomix

$paleomix
```

Picard Tools - The latest version compatable with PaleoMix is v1.1.37, but the earliest version on the server is Picard 2.20. 



## Map and Process bams in qrsh

## Mapping

Map to reference genome: 
```
cat 02a_MapwithBWAmem.ARRAY_museum_TrimTest.sh
#!/bin/bash
#$ -S /bin/bash
#$ -N C3.BWAmem_mod  ##job name
#$ -l tmem=16G #RAM
#$ -l h_vmem=16G #enforced limit on memory shell usage
#$ -l h_rt=10:00:00 ##wall time.  
#$ -j y  #concatenates error and output files (with prefix job1)

#run job in working directory
cd $SGE_O_WORKDIR 


##Software
BWA=/share/apps/genomics/bwa-0.7.17/bwa
export PATH=/share/apps/genomics/samtools-1.9/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/genomics/samtools-1.9/lib:$LD_LIBRARY_PATH

#Define variables
SHAREDFOLDER=/SAN/ugi/LepGenomics
SPECIES=E3_Aphantopus_hyperantus
REF=$SHAREDFOLDER/$SPECIES/RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna
INPUT=$SHAREDFOLDER/$SPECIES/TrimmomaticTest
OUTPUT=$SHAREDFOLDER/$SPECIES/TrimmomaticTest
NAME=AH-01-1900-02



##Check if Ref Genome is indexed by bwa
if [[ ! $REF.fai ]]
then 
	echo $REF" not indexed. Indexing now"
	$BWA index $REF
else
	echo $REF" indexed"
fi


##Map 

echo "time $BWA mem $REF $INPUT/$NAME | samtools sort -o  $OUTPUT/$NAME.bam" >> map_mus.log
time $BWA mem $REF $INPUT/$NAME.collapsed | samtools sort -o  $OUTPUT/$NAME.bam
```


Bam processing: AddRG, MarkDup, LocalRealn, CheckBam
```
#Index bam
samtools=/share/apps/genomics/samtools-1.9/bin/samtools
$samtools index AH-01-1900-02.bam

#Bam processing
export PATH=/share/apps/java/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/java/lib:$LD_LIBRARY_PATH
PICARD=/share/apps/genomics/picard-2.20.3/bin/picard.jar

#Add RG info
time java -jar $PICARD AddOrReplaceReadGroups \
       I=AH-01-1900-02.bam \
       O=AH-01-1900-02.collapsed.RG.bam \
       RGID=E3mus \
       RGLB=mus0204 \
       RGPL=ILLUMINA \
       RGPU=unit1 \
       RGSM=AH02

#MarkDuplicates
time java -jar $PICARD MarkDuplicates \
INPUT=AH-01-1900-02.collapsed.RG.bam \
OUTPUT=AH-01-1900-02.collapsed.rmdup.bam \
METRICS_FILE=AH-01-1900-02.dup.txt \
REMOVE_DUPLICATES=false \
VALIDATION_STRINGENCY=SILENT \
CREATE_INDEX=true


#Local Realignment
export PATH=/share/apps/jdk1.8.0_131/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/jdk1.8.0_131/lib:$LD_LIBRARY_PATH
GenomeAnalysisTK=/share/apps/genomics/GenomeAnalysisTK-3.8.1.0/GenomeAnalysisTK.jar
REF=/SAN/ugi/LepGenomics/E3_Aphantopus_hyperantus/RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna

###Identify targets to realign
java -jar $GenomeAnalysisTK -T RealignerTargetCreator \
-R $REF \
-o AH-01-1900-02.intervals \
-I AH-01-1900-02.collapsed.rmdup.bam

###Local realignment
java -jar $GenomeAnalysisTK -T IndelRealigner \
-R $REF \
-targetIntervals AH-01-1900-02.intervals \
-I AH-01-1900-02.collapsed.rmdup.bam \
-o AH-01-1900-02.realn.bam

#Validate Bam
time java -jar $PICARD ValidateSamFile \
INPUT=AH-01-1900-02.realn.bam \
OUTPUT=AH-01-1900-02.validatesam \
MODE=SUMMARY

```

MapDamage
```
#Submit script

#$ -S /bin/bash
#$ -N E3.MapDmg.mus  ##job name
#$ -l tmem=32G #RAM
#$ -l h_vmem=32G #enforced limit on shell memory usage
#$ -l h_rt=5:00:00 ##wall time.  
#$ -j y  #concatenates error and output files (with prefix job1)
##$ -t 1-48

#Run on working directory
cd $SGE_O_WORKDIR


# Software
##python
export PATH=/share/apps/python-3.8.5-shared/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/python-3.8.5-shared/lib:$LD_LIBRARY_PATH

##R
export PATH=/share/apps/R-4.0.3/bin:$PATH

##mapDamage
mapDamage="/share/apps/python-3.8.5-shared/bin/mapDamage"


# Define variables
SHAREDFOLDER=/SAN/ugi/LepGenomics
SPECIES=E3_Aphantopus_hyperantus
REF=$SHAREDFOLDER/$SPECIES/RefGenome/GCA_902806685.1_iAphHyp1.1_genomic.fna
#INPUT=$SHAREDFOLDER/$SPECIES/02a_mapped_museum_FORANGSD
#OUTPUT=$SHAREDFOLDER/$SPECIES/02a_mapped_museum_FORANGSD/MAPDAMAGE
INPUT=$SHAREDFOLDER/$SPECIES/TrimmomaticTest
OUTPUT=$SHAREDFOLDER/$SPECIES/TrimmomaticTest
TAIL="realn.bam"


#Create Array
#NAME=$(sed "${SGE_TASK_ID}q;d" mus.names)
NAME=AH-01-1900-02.realn.bam

##Script
echo "time $mapDamage --merge-libraries -i $INPUT/${NAME}.$TAIL -d $OUTPUT -r $REF --rescale --single-stranded"
time $mapDamage --merge-libraries -i $INPUT/$NAME -d $OUTPUT/$NAME -r $REF --rescale --single-stranded  
```





